---
layout: distill
title: Augmenting Imitation Experience via Equivariant Representations
description: Performing data augmentation via generated image embeddings towards better navigation for autonomous vehicles.
date: 2020-11-08
authors:
  - name: Dhruv Sharma
    affiliations: 
      name: University of Toronto
  - name: Ali Kuwajerwala
    url: "https://alik-git.github.io/"
  - name: Florian Shkurti
    url: "http://www.cs.toronto.edu/~florian/"
---

# In a Nutshell

We learn  an  embedding  method  that  is  equivariant with respect to relative camera pose or viewpoint, and that allows us to predict the embeddings from other nearby viewpoints without visiting or measuring them directly. For instance, given an image from a camera mounted at the center of a self driving car, we aim to predict what embedding space would look like if the image came from a camera mounted on the left of the self driving car. Hence, without visiting or taking measurements from a viewpoint, we aim to predict what the embedding would look like.


### [Paper]()

## Problem Setting

To achieve robustness in imitation learning from image for autonmous navigation, a common method is to augment the training data with image-action pairs obtained from additional cameras. For instance, for a self driving car, prior research has employed data collecetd from off-center cameras mounted on a vehicle. This method requires additional sensors, compute, and eventually leads to higher costs. Our objective is to use prior knowledge about the scene to predict representations for different sensor viewpoints and use it to augment training of an imitation learning policy. This way we can reduce reliance on additional sensors, while at the same time improving imitaiton performance.

## Our Method

Our method uses the concept of image equivariance. Equivariance for image representations captures to what degree are the encodings of two related images determined by the transformation of those images. If the encodings are the same, then they are invariant to that transformation. In our work, we are dealing with image pairs that look at the same scene from different, but nearby viewpoints, for instance the center and the left camera mounted on a self-driving car. We want the representation of the left camera to be predictable by the center camera, so we want a high degree of equivariance. 

<div>
  <img src="/assets/img/eqRepLearning/eqTrainingFlow.png"  alt="method diag">
  <p></p>
</div>

We learn an embedding that is equivariant with respect to input image viewpoint. To learn such an embedding we train an autoencoder network that is optimized for an equivariance objective as seen in above figure. We collect data using multiple sensors and use it to learn such embeddings.

## Imitation Learning Using Learned Equivariant Embeddings

<div>
  <img src="/assets/img/eqRepLearning/controlLearning.png"  alt="method imitation learning">
  <p></p>
</div>

We use these learnt embeddings to train an immitation learning policy to control an autonomous vehicle. Such policy takes in image as the input and predicts control actions as output. Above figure shows the training process for learning imitation policy using equivariant representations.


## Experiments 

We perform experiments in simulation and in real life in a ground vehicle and an aerial vehicle setting. Our experiments aim to measure the effectiveness of training using equivariant augmentations. To do so, we compare the performance of imitation learning policy trained using equivariant augmentations with policies trained without any augmentations (image-action pairs obtained from single camera) and policies trained with augmentations (image-action pairs obtained from multiple cameras)from multiple camera sensors.

Through our experiments we showcase that our method helps to improve performance when compared to policy trained using a single camera while lagging behind policy trained using multiple cameras.

Here is a video from our ground robot experiments showcasing the results on a clearpath husky robot.

<iframe width="560" height="315" src="https://www.youtube.com/embed/5g4Kg3-YWvA" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> 


***

For more extensive method details and results, please check out our paper!


<!-- Effective planning in model-based reinforcement learning (MBRL) and model-predictive control (MPC) relies on the accuracy of the learned dynamics model. In many instances of MBRL and MPC, this model is assumed to be stationary and is periodically re-trained from scratch on state transition experience collected from the beginning of environment interactions. This implies that the time required to train the dynamics model -- and the pause required between plan executions -- grows linearly with the size of the collected experience. We argue that this is too slow for lifelong robot learning and propose **HyperCRL, a method that continually learns the encountered dynamics in a sequence of tasks using task-conditional hypernetworks.** 

Our method has three main attributes: first, it **enables constant-time dynamics learning sessions between planning and only needs to store the most recent fixed-size portion of the state transition experience; second, it uses fixed-capacity hypernetworks to represent non-stationary and task-aware dynamics; third, it outperforms existing continual learning alternatives that rely on fixed-capacity networks, and does competitively with baselines that remember an ever increasing coreset of past experience.** We show that HyperCRL is effective in continual model-based reinforcement learning in robot locomotion and manipulation scenarios, such as tasks involving pushing and door opening.

### [Code](https://github.com/philip-huang/HyperCRL) $~$  [Paper]()

## Problem Setting

We consider the following problem setting: A robot interacts with the environment to solve a sequence of $T$ goal-directed tasks, each of which brings about different dynamics while having the same state-space $S$ and action space $A$. The robot is exposed to the tasks sequentially online without revisiting data collected in a previous task.

## Our method
<div>
  <img src="/assets/img/hypercrl_diag.png"  alt="method diag" width="700" height="280">
  <p></p>
</div>

We have a learned dynamics model, the parameters of which are inferred through a task-conditioned hypernetwork. Given learned task embeddings and parameters of the hypernetwork, we infer parameters of the dynamics neural network. Using this dynamics model, we perform CEM optimization to generate action sequences and execute them in the environment for $K$ time-steps with MPC. We store the observed transitions in the replay dataset and update the parameters of the hypernetwork and task-embeddings. We repeat this for $M$ episodes per task, and for each of the $T$ tasks sequentially.

## Video

### Door-opening with a Panda arm (2x real time)

<iframe width="560" height="315" src="https://www.youtube.com/embed/gsmLhP8WfKM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

A panda arm must be controlled to open a door. The reward function is designed such that the agent receives higher reward for opening the door to a wider angle. The agent is controlled using an operational space controller (both position and orientation). The different tasks correspond to different shapes of the door knob.

### Pushing a non-uniform cube with a Panda arm (2x real time)

<iframe width="560" height="315" src="https://www.youtube.com/embed/uQlqBhXK-28" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

A panda arm must be controlled to push a block to a goal location without changing the oreintation of the block. The agent is controlled using an operational space controller (only position. orientation of the end-effector is fixed.). The different tasks correspond to different friction coefficients between the cube and the two sides of the table top

***

For more details, please check our paper! -->