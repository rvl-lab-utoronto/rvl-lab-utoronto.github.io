(this["webpackJsonprvl-lab-utoronto"]=this["webpackJsonprvl-lab-utoronto"]||[]).push([[0],{127:function(e,t,n){},136:function(e,t,n){},139:function(e,t,n){},140:function(e,t,n){"use strict";n.r(t),t.default=n.p+"static/media/bars-solid.e8db7520.svg"},355:function(e,t,n){},356:function(e,t,n){},357:function(e,t,n){},358:function(e,t,n){},359:function(e,t,n){"use strict";n.r(t),t.default=n.p+"static/media/florian.ef061895.jpg"},360:function(e,t,n){},369:function(e,t,n){"use strict";n.r(t),t.default=n.p+"static/media/caret-left-solid.f3619997.svg"},370:function(e,t,n){"use strict";n.r(t),t.default=n.p+"static/media/caret-right-solid.572aedcd.svg"},386:function(e,t,n){},387:function(e,t,n){},388:function(e,t,n){},391:function(e,t,n){},397:function(e,t,n){},398:function(e,t,n){"use strict";n.r(t),t.default=n.p+"static/media/envelope-solid.80b23b18.svg"},399:function(e,t,n){"use strict";n.r(t),t.default=n.p+"static/media/twitter-brands.87682341.svg"},40:function(e,t,n){e.exports={list:"faq_list__1jVJg",info:"faq_info___ASGj",accordion:"faq_accordion__1MIB2",item:"faq_item__Jaq1A",itemBtn:"faq_itemBtn__3Mqiu",itemBtnExpanded:"faq_itemBtnExpanded__1aqFo",itemContent:"faq_itemContent__5Ptuz",itemPanel:"faq_itemPanel__32AvB",chevron:"faq_chevron__2Yqij"}},400:function(e,t,n){"use strict";n.r(t),t.default=n.p+"static/media/linkedin-brands.f4041044.svg"},401:function(e,t,n){"use strict";n.r(t),t.default=n.p+"static/media/graduation-cap-solid.af8ffce9.svg"},402:function(e,t,n){"use strict";n.r(t);var a=n(1),i=n.n(a),o=n(33),r=n.n(o),s=(n(127),n(11)),c=n(21),l=n(15),d=n(440),h=n(433),p=n(4),u=n(5),b=n(7),m=n(8),g=n(0),j=function(e){Object(b.a)(n,e);var t=Object(m.a)(n);function n(){return Object(p.a)(this,n),t.apply(this,arguments)}return Object(u.a)(n,[{key:"componentDidUpdate",value:function(e){this.props.location.pathname!==e.location.pathname&&window.scrollTo(0,0)}},{key:"render",value:function(){return Object(g.jsx)("div",{})}}]),n}(i.a.Component),f=Object(l.g)(j),v=n(48),y=n(44),x=n(58),w=(n(136),n(431)),O=n(113),k=n(112),S=(n(137),n(98)),C=n.n(S),I=n(99),_=(n(139),[{link:"https://www.youtube.com/channel/UCrDIJSrZ5I46topcN6MoF7g",name:"YouTube",icon:"assets/social-icons/youtube.png"},{link:"https://twitter.com/florian_shkurti",name:"Twitter",icon:"assets/social-icons/twitter.png"},{link:" https://github.com/rvl-lab-utoronto",name:"GitHub",icon:"assets/social-icons/github.png"},{link:"https://vectorinstitute.ai/",name:"Vector Institute",icon:"assets/social-icons/vector institute.png"},{link:"https://robotics.utoronto.ca/",name:"UofT Robotics Institute",icon:"assets/social-icons/university.png"},{link:"https://robotics.cs.toronto.edu/",name:"CS Robotics",icon:"assets/social-icons/robot-arm.png"}]),A=function(e){Object(b.a)(a,e);var t=Object(m.a)(a);function a(){var e;return Object(p.a)(this,a),(e=t.call(this)).handlePageChange=function(t){var n=e.getCurrentPageName(t);e.setState({currentLink:t,currentName:n.title,open:!1})},e.getCurrentPageName=function(e){for(var t=Object.keys(Ge),n=0;n<t.length;n++)for(var a=0;a<Ge[t[n]].length;a++)if(e===Ge[t[n]][a].link)return{title:Ge[t[n]][a].title,category:t[n]};return{title:"",category:""}},e.navbarPages=Ge,e.navbarPagesTotal=Object(s.a)(e.navbarPages.main),e.state={open:!1,currentLink:"",currentName:""},e.firstOpen=!0,e.state={open:!1},e}return Object(u.a)(a,[{key:"render",value:function(){var e=this;return Object(g.jsxs)("div",{className:"navbar",children:[Object(g.jsx)("div",{className:"desktop-view",children:Object(g.jsxs)("div",{className:"navbar-flex horizontal-padding max-width-home",children:[Object(g.jsx)("div",{style:{width:"250px"},children:Object(g.jsx)(c.b,{to:"/",children:Object(g.jsx)("img",{alt:"RVL",className:"rvl-icon-desktop",src:n(81).default})})}),Object(g.jsx)("div",{children:this.navbarPagesTotal.map((function(t,n){return Object(g.jsx)(T,{selected:e.state.currentLink===t.link,title:t.title,link:t.link})}))}),Object(g.jsx)("div",{className:"navbar-socials",style:{width:"250px"},children:_.map((function(e,t){return Object(g.jsx)(g.Fragment,{children:Object(g.jsx)(M,{social:e},e.name)})}))})]})}),Object(g.jsxs)("div",{className:"mobile-view",children:[Object(g.jsxs)("div",{className:"navbar-flex",style:{zIndex:100,backgroundColor:"white"},children:[Object(g.jsx)(c.b,{to:"/",children:Object(g.jsx)("img",{alt:"RVL",className:"rvl-icon-mobile",src:n(81).default})}),Object(g.jsx)("div",{style:{height:"50px"}}),Object(g.jsx)("div",{className:"navbar-socials",style:{position:"absolute",right:"38px",top:"10px"},children:_.map((function(e,t){return Object(g.jsx)(g.Fragment,{children:Object(g.jsx)(M,{social:e},e.name)})}))}),Object(g.jsx)("img",{onClick:function(){e.setState({open:!e.state.open})},alt:"menu",className:"navbar-menu-icon",src:n(140).default})]}),Object(g.jsx)("div",{className:"navbar-items-mobile "+(this.state.open?"":"navbar-items-mobile-open"),children:this.navbarPagesTotal.map((function(t,n){return Object(g.jsx)(R,{selected:e.state.currentLink===t.link,title:t.title,link:t.link})}))})]})]})}}]),a}(a.Component),T=function(e){Object(b.a)(n,e);var t=Object(m.a)(n);function n(){return Object(p.a)(this,n),t.apply(this,arguments)}return Object(u.a)(n,[{key:"render",value:function(){return Object(g.jsx)(c.b,{to:this.props.link,className:"navbar-link-text "+(this.props.selected?"navbar-link-text-selected":""),children:this.props.title})}}]),n}(a.Component),R=function(e){Object(b.a)(n,e);var t=Object(m.a)(n);function n(){return Object(p.a)(this,n),t.apply(this,arguments)}return Object(u.a)(n,[{key:"render",value:function(){return Object(g.jsx)(c.b,{to:this.props.link,className:"navbar-link-text-mobile "+(this.props.selected?"navbar-link-text-selected-mobile":""),children:this.props.title})}}]),n}(a.Component),M=function(e){Object(b.a)(n,e);var t=Object(m.a)(n);function n(){return Object(p.a)(this,n),t.apply(this,arguments)}return Object(u.a)(n,[{key:"render",value:function(){return Object(g.jsx)("a",{className:"navbar-social",href:this.props.social.link,style:{textDecorationColor:"transparent"},children:Object(g.jsx)("img",{alt:this.props.social.name,className:"navbar-social-image",src:"/rvl-lab-utoronto.github.io/"+this.props.social.icon})})}}]),n}(a.Component),P=function(e){Object(b.a)(n,e);var t=Object(m.a)(n);function n(){return Object(p.a)(this,n),t.apply(this,arguments)}return Object(u.a)(n,[{key:"render",value:function(){return Object(g.jsxs)(g.Fragment,{children:[Object(g.jsx)("div",{className:"mobile-view",children:Object(g.jsx)("div",{style:{marginTop:"80px"}})}),Object(g.jsx)("div",{className:"desktop-view",children:Object(g.jsx)("div",{style:{marginTop:"80px"}})})]})}}]),n}(a.Component),L=n(436),D=n(430),E=function(e){Object(b.a)(n,e);var t=Object(m.a)(n);function n(){return Object(p.a)(this,n),t.apply(this,arguments)}return Object(u.a)(n,[{key:"render",value:function(){var e=Object(g.jsxs)(g.Fragment,{children:[Object(g.jsx)("div",{style:{height:"5px"}}),Object(g.jsx)("h2",{children:this.props.blog.title}),this.props.blog.date?Object(g.jsx)("p",{style:{margin:"1px",marginTop:"5px",marginBottom:"5px",color:"black"},children:this.props.blog.date}):Object(g.jsx)(g.Fragment,{}),this.props.blog.description?Object(g.jsx)("p",{style:{margin:"1px",color:"black"},children:this.props.blog.description}):Object(g.jsx)(g.Fragment,{}),Object(g.jsx)("div",{style:{height:"25px"}}),Object(g.jsx)("hr",{})]});return void 0!==this.props.blog.asset&&""!==this.props.blog.asset&&void 0!==this.props.blog.webLocation&&""!==this.props.blog.webLocation?Object(g.jsx)(c.b,{className:"blog-entry-link",to:"/blog/"+this.props.blog.webLocation,children:e}):void 0!==this.props.blog.link&&""!==this.props.blog.link?Object(g.jsx)("a",{className:"blog-entry-link",href:this.props.blog.link,children:e}):e}}]),n}(a.Component);function N(e){var t=e.replaceAll('"assets/','"/rvl-lab-utoronto.github.io/assets/');return t=t.replaceAll("'assets/","'/rvl-lab-utoronto.github.io/assets/")}var F=function(e){Object(b.a)(n,e);var t=Object(m.a)(n);function n(e){var a;return Object(p.a)(this,n),(a=t.call(this,e)).state={text:""},a}return Object(u.a)(n,[{key:"componentDidMount",value:function(){var e=Object(x.a)(Object(v.a)().mark((function e(){var t,n,a,i,o,r,s,c,l,d;return Object(v.a)().wrap((function(e){for(;;)switch(e.prev=e.next){case 0:if(this.props.distill){e.next=14;break}return e.next=3,fetch(this.props.src);case 3:return t=e.sent,e.next=6,t.text();case 6:n=e.sent,a=N(a=Object(I.decode)(C.a.renderToString(Object(g.jsx)(g.Fragment,{children:Object(g.jsx)(w.a,{remarkPlugins:[O.a],rehypePlugins:[k.a],children:n})})))),i=a.split("<pre><code>"),o=[i[0]],r=Object(y.a)(i.splice(1));try{for(r.s();!(s=r.n()).done;)c=s.value,l=("<pre><code>"+c).split("</code></pre>"),d=l[0].trimEnd(),o.push(d),o.push(l[1])}catch(h){r.e(h)}finally{r.f()}this.setState({htmlSplit:o});case 14:case"end":return e.stop()}}),e,this)})));return function(){return e.apply(this,arguments)}}()},{key:"render",value:function(){return this.props.distill?Object(g.jsxs)(g.Fragment,{children:[Object(g.jsx)("div",{style:{height:"55px"}}),Object(g.jsx)("iframe",{id:"iframe",style:{width:"100vw",height:"calc(100vh - 8px - 55px)"},title:"blogPost",src:this.props.src})]}):""!==this.state.htmlSplit?Object(g.jsxs)(g.Fragment,{children:[Object(g.jsx)(P,{}),Object(g.jsx)("div",{children:Object(g.jsx)("div",{className:"center",children:Object(g.jsxs)("div",{className:"horizontal-padding max-width-blog blog-entry-page",children:[!0===this.props.removeExtraSpace?Object(g.jsx)("div",{}):Object(g.jsx)("div",{style:{height:"25px"}}),void 0!==this.props.articleData?Object(g.jsxs)(g.Fragment,{children:[Object(g.jsx)("h1",{style:{fontWeight:600},children:null===(e=this.props.articleData)||void 0===e?void 0:e.title}),void 0!==this.props.articleData.authors||void 0!==this.props.articleData.date||void 0!==this.props.articleData.affiliations?Object(g.jsxs)(g.Fragment,{children:[Object(g.jsx)("div",{style:{height:"20px"}}),Object(g.jsx)("hr",{}),Object(g.jsx)("div",{style:{height:"10px"}}),Object(g.jsxs)("div",{className:"article-data",children:[void 0!==this.props.articleData.authors?Object(g.jsxs)("div",{style:{display:"flex",flexDirection:"column"},children:[Object(g.jsx)("h4",{className:"article-data-header",children:"Authors"}),null===(t=this.props.articleData)||void 0===t||null===(n=t.authors)||void 0===n?void 0:n.map((function(e){return Object(g.jsx)("h3",{className:"article-data-label",children:e})}))]}):Object(g.jsx)(g.Fragment,{}),void 0!==this.props.articleData.affiliations?Object(g.jsxs)("div",{style:{display:"flex",flexDirection:"column"},children:[Object(g.jsx)("h4",{className:"article-data-header",children:"Affiliations"}),null===(a=this.props.articleData)||void 0===a||null===(i=a.affiliations)||void 0===i?void 0:i.map((function(e){return Object(g.jsx)("h3",{className:"article-data-label",children:e})}))]}):Object(g.jsx)(g.Fragment,{}),void 0!==this.props.articleData.date?Object(g.jsxs)("div",{style:{display:"flex",flexDirection:"column"},children:[Object(g.jsx)("h4",{className:"article-data-header",children:"Published"}),Object(g.jsx)("h3",{className:"article-data-label",children:null===(o=this.props.articleData)||void 0===o?void 0:o.date})]}):Object(g.jsx)(g.Fragment,{})]}),Object(g.jsx)("div",{style:{height:"10px"}}),Object(g.jsx)("hr",{})]}):Object(g.jsx)(g.Fragment,{}),Object(g.jsx)("div",{style:{height:"16px"}})]}):Object(g.jsx)(g.Fragment,{}),this.state.htmlSplit&&this.state.htmlSplit.map((function(e){if(e.startsWith("<pre><code>")){var t=e.split("<pre><code>")[1];return Object(g.jsx)(L.a,{language:"javascript",style:D.a,children:t})}return Object(g.jsx)("div",{dangerouslySetInnerHTML:{__html:e}})}))]})})})]}):Object(g.jsx)(g.Fragment,{});var e,t,n,a,i,o}}]),n}(a.Component),G=(n(355),function(e){Object(b.a)(n,e);var t=Object(m.a)(n);function n(){return Object(p.a)(this,n),t.apply(this,arguments)}return Object(u.a)(n,[{key:"render",value:function(){return Object(g.jsx)("div",{className:"footer",children:Object(g.jsx)("p",{className:"accent-paragraph footer-copyright",style:{color:"gray"},children:"\n  \xa9 Robot Vision & Learning (RVL) Lab. All rights reserved. \n"})})}}]),n}(a.Component)),H=[{title:"Learning to Search in Task and Motion Planning with Streams",date:"September 14, 2021",link:"https://rvl.cs.toronto.edu/learning-based-tamp/"},{title:"Continual Model-Based Reinforcement Learning with Hypernetworks",date:"August 18, 2020",webLocation:"hypercrl-md",asset:"assets/blog-pages/hypercrl.md",articleData:{title:"Continual Model-Based Reinforcement Learning with Hypernetworks",date:"August 18, 2020",authors:["Yizhou (Philip) Huang","Kevin Xie","Homanga Bharadhwaj","Florian Shkurti"],affiliations:["University of Toronto"]}}],z=function(e){Object(b.a)(n,e);var t=Object(m.a)(n);function n(){return Object(p.a)(this,n),t.apply(this,arguments)}return Object(u.a)(n,[{key:"render",value:function(){return Object(g.jsx)("div",{className:"center",children:Object(g.jsxs)("div",{className:"horizontal-padding max-width",children:[Object(g.jsxs)("div",{style:{minHeight:"100vh"},children:[Object(g.jsx)(P,{}),Object(g.jsx)("div",{style:{height:"1px"}}),H.map((function(e){return Object(g.jsx)(E,{blog:e})}))]}),Object(g.jsx)("div",{style:{position:"absolute",left:0,width:"100vw"},children:Object(g.jsx)(G,{})})]})})}}]),n}(a.Component),V=(n(356),n(357),[{title:"autonomous robots for environmental monitoring",content:[],asset:"assets/research-themes/autonomous robots.jpg",web:"/research/autonomous-robots"},{title:"learning to plan, perceive, and control",content:[],asset:"assets/research-themes/machine learning.jpg",web:"/research/machine-learning"},{title:"safe robot learning",content:[],asset:"assets/research-themes/safe robot learning.jpg",web:"/research/safe-robot"}]),B=function(e){Object(b.a)(n,e);var t=Object(m.a)(n);function n(){return Object(p.a)(this,n),t.apply(this,arguments)}return Object(u.a)(n,[{key:"render",value:function(){var e=this;return Object(g.jsx)(g.Fragment,{children:Object(g.jsxs)("div",{className:"research-themes",children:[Object(g.jsx)("h2",{className:"research-theme-title",children:"Research Themes"}),Object(g.jsx)("div",{className:"research-themes-box-container",children:V.map((function(t,n){return void 0===e.props.indexesToShow||e.props.indexesToShow.includes(n)?Object(g.jsx)(g.Fragment,{children:Object(g.jsx)(U,{themes:V[n].content,theme:t})}):Object(g.jsx)("div",{})}))})]})})}}]),n}(a.Component),U=function(e){Object(b.a)(n,e);var t=Object(m.a)(n);function n(){return Object(p.a)(this,n),t.apply(this,arguments)}return Object(u.a)(n,[{key:"render",value:function(){return Object(g.jsxs)("div",{className:"research-themes-box",children:[Object(g.jsx)("h3",{children:Object(g.jsx)(c.b,{to:this.props.theme.web,children:this.props.theme.title})}),this.props.themes.map((function(e,t){return Object(g.jsx)("p",{children:e})}))]})}}]),n}(a.Component),J=(n(358),function(e){Object(b.a)(a,e);var t=Object(m.a)(a);function a(){return Object(p.a)(this,a),t.apply(this,arguments)}return Object(u.a)(a,[{key:"render",value:function(){return Object(g.jsx)(g.Fragment,{children:Object(g.jsxs)("div",{className:"lab-intro-box-container",children:[Object(g.jsx)("div",{className:"lab-intro-box",children:Object(g.jsx)("img",{src:n(359).default,alt:""})}),Object(g.jsxs)("div",{className:"lab-intro-box-text",children:[Object(g.jsx)("h2",{children:"Welcome"}),Object(g.jsxs)("p",{children:[" Welcome to the Robot Vision and Learning (RVL) lab. We are part of the ",Object(g.jsx)("a",{href:"https://web.cs.toronto.edu/",children:"Computer Science"})," department at the ",Object(g.jsx)("a",{href:"https://www.utoronto.ca/",children:"University of Toronto"}),", the ",Object(g.jsx)("a",{href:"https://www.utm.utoronto.ca/math-cs-stats/",children:"MCS"})," department at ",Object(g.jsx)("a",{href:"https://www.utm.utoronto.ca",children:"UTM"}),", and the ",Object(g.jsx)("a",{href:"https://robotics.utoronto.ca/",children:"UofT Robotics Institute"}),". The group is led by ",Object(g.jsx)("a",{href:"http://www.cs.toronto.edu/~florian/",children:"Prof. Florian Shkurti"}),", and consists of students with backgrounds in robotics, machine learning, engineering, control theory, and physics."]}),Object(g.jsx)("p",{children:"We develop methods that enable robots to perceive, reason, and act effectively and safely, particularly in dynamic environments and alongside humans. Application areas include field robotics for environmental monitoring, visual navigation for autonomous vehicles, and mobile manipulation."})]})]})})}}]),a}(a.Component)),K=(n(360),[{date:"2023-06-1",content:"Our paper on open-set 3D mapping, <a href='https://concept-fusion.github.io/'>ConceptFusion</a>, got accepted at RSS. Congratulations to Krishna Murthy and the wonderful co-authors, who also demoed the method live at CVPR."},{date:"2023-04-1",content:"The <a href='https://acceleration.utoronto.ca/'>Acceleration Consortium</a> won a <a href=''>$200M grant</a> to accelerate materials discovery using chemistry, machine learning, lab automation and robotics. <a href='https://acceleration.utoronto.ca/news/were-hiring'>We're hiring staff scientists, postdocs, graduate, and undergraduate students</a>."},{date:"2023-02-1",content:"Our field robotics paper on <a href='https://arxiv.org/abs/2209.11864'>vision-based navigation for autonomous boats</a> got accepted at ICRA. This is a collaboration with Tim Barfoot. Philip Huang, who led the work, graduated from his MSc and will start his PhD at CMU."},{date:"2023-02-1",content:"Two papers accepted at CVPR, one on <a href='https://arxiv.org/abs/2303.14595'>continual learning of neural networks</a>, led by Qiao Gu, and another on <a href='https://arxiv.org/abs/2303.13755'>sparsifying vision transformers</a>, led by Cong Wei."},{date:"2023-02-1",content:"Two papers on learning for task and motion planning got accepted at <a href='https://arxiv.org/abs/2210.14055'>ICRA</a> and <a href='https://arxiv.org/abs/2111.13144'>RA-L</a>. Mohamed Khodeir, the first author, graduated from his MSc."},{date:"2023-02-1",content:"Many thanks to <a href='https://scholar.google.com/citations?user=5HHtXzwAAAAJ'>Michal Zajac</a> and David Helm for visiting RVL for 4 and 6 months respectively. It was a pleasure hosting them."},{date:"2022-03-1",content:"Our paper on <a href='https://openaccess.thecvf.com/content/CVPR2022/html/Khorasgani_SLIC_Self-Supervised_Learning_With_Iterative_Clustering_for_Human_Action_Videos_CVPR_2022_paper.html'>video representation learning</a> was accepted to CVPR for oral presentation. "},{date:"2022-02-1",content:"Our paper on <a href='https://arxiv.org/abs/2110.07668'>equivariant representations for imitation learning</a> was accepted to ICRA. "},{date:"2021-08-12",content:"Two papers accepted at CoRL, one on <a href='https://openreview.net/forum?id=nWLt35BU1z_'>task planning in large 3D scene graphs</a>, and one on <a href='https://openreview.net/forum?id=tCfLLiP7vje'>perceiving transparent objects from RGBD sensors</a> (oral). "},{date:"2021-07-07",content:"Our paper on <a href='https://nv-tlabs.github.io/physics-pose-estimation-project-page/'>physics-based human motion tracking and synthesis from videos</a> was accepted at ICCV.  "},{date:"2021-05-07",content:"Florian received an <a href='https://www.amazon.science/research-awards/recipients?f0=2020&f1=00000173-2161-da60-a1f3-b9f59a740001&s=0'>Amazon Research Award</a>. "},{date:"2021-02-28",content:"Three papers accepted at ICRA, one on <a href='https://arxiv.org/abs/2009.11997'>continual model-based RL</a>, one on <a href='https://arxiv.org/abs/2005.10934'>reachability based exploration in RL</a>, and one on <a href='https://arxiv.org/abs/2011.01298'>handling imperfect demonstrations in imitation and RL</a>. "},{date:"2021-02-28",content:"One paper accepted at CVPR, <a href='https://arxiv.org/abs/2103.03891'>on latent space disentanglement in GANs for image synthesis</a>. "},{date:"2021-01-12",content:"Three papers accepted at ICLR, one on <a href='https://openreview.net/forum?id=iaO86DUuKi'>safe reinforcement learning</a>, one on <a href='https://openreview.net/forum?id=jXe91kq3jAq'>learning transferable skills for hierarchical planning</a>, and one on <a href='https://openreview.net/forum?id=c_E8kFWfhp0'>differentiable physics and rendering simulators</a>. "},{date:"2020-12-2",content:"Florian is co-organizing the NeurIPS 2020 <a href='https://montrealrobotics.ca/diffcvgp/'>workshop on differentiable computer vision, graphics, and physics</a>. "},{date:"2020-12-2",content:"Our <a href='https://arxiv.org/abs/2003.04514'>paper</a> on encouraging diversity in neural network ensembles was accepted at AAAI. "},{date:"2020-07-20",content:"<a href='https://www.youtube.com/watch?v=AeMSp-_hSnU'>Our work</a>, led by collaborators <a href='http://www.cim.mcgill.ca/~travism/'>Travis</a>, <a href='http://www.cim.mcgill.ca/~gamboa/'>Juan Camilo</a>, <a href='https://ca.linkedin.com/in/stefan-wapnick-00b122a6'>Stefan</a> and others, won the Best Paper Award in the RSS'20 Workshop on Self-Supervised Robot Learning."},{date:"2020-07-2",content:"Two papers accepted at IROS. One on <a href='https://arxiv.org/abs/2003.10010'>visual search</a> and one on <a href='https://arxiv.org/abs/2003.07489'>mobile manipulation</a>.   "},{date:"2020-06-05",content:"Florian co-organized the <a href='https://starslab.ca/workshops/icra_2020_debates/'>Debates on the Future of Robotics Research</a> workshop at ICRA 2020."},{date:"2020-05-07",content:"<a href='http://www.roboticsproceedings.org/rss16/p048.html'>Our paper on vision-based navigation</a> was accepted at RSS. "}]),q=function(e){Object(b.a)(a,e);var t=Object(m.a)(a);function a(){var e;return Object(p.a)(this,a),(e=t.call(this)).amountShow=10,e.amountShowExpand=10,e.state={show:e.amountShow},e}return Object(u.a)(a,[{key:"render",value:function(){var e=this;return Object(g.jsxs)(g.Fragment,{children:[Object(g.jsx)("h2",{className:"news-title",children:"News"}),Object(g.jsxs)("div",{className:"news-box-container",children:[K.map((function(t,n){return n>=e.state.show?Object(g.jsx)(g.Fragment,{}):Object(g.jsx)("div",{className:"news-box",children:Object(g.jsx)("p",{dangerouslySetInnerHTML:{__html:'<span class="boxed">'+X(t.date)+" "+W(t.date)+"</span> "+t.content}})})})),this.state.show>=K.length?Object(g.jsx)(g.Fragment,{}):Object(g.jsx)("div",{style:{width:"100%",marginTop:"10px"},className:"center",children:Object(g.jsx)("div",{onClick:function(){e.setState({show:e.state.show+e.amountShowExpand})},className:"news-load-more-button",children:Object(g.jsx)("img",{alt:"more",style:{height:"24px",width:"24px"},src:n(73).default})})})]})]})}}]),a}(a.Component);function W(e){return void 0===e||3!==e.split("-").length?"":e.split("-")[0]}var Y=["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sept","Oct","Nov","Dec"];function X(e){return void 0===e||3!==e.split("-").length?"":Y[parseInt(e.split("-")[1])-1]}var Z=n(6),Q=(n(361),n(107)),$=["assets/slideshow/husky_utm_2020.png","assets/slideshow/arm_farm.jpg","assets/slideshow/utah_flying_1.png","assets/slideshow/boat_1.jpg","assets/slideshow/boat_2.jpg","assets/slideshow/visual_search_diver_tracking_iros_2020.png","assets/slideshow/ball_catching_ke.gif"],ee=function(e){Object(b.a)(a,e);var t=Object(m.a)(a);function a(){return Object(p.a)(this,a),t.apply(this,arguments)}return Object(u.a)(a,[{key:"render",value:function(){return Object(g.jsx)(Q.Carousel,Object(Z.a)({infiniteLoop:!0,children:!1,labels:!1,showStatus:!1,showThumbs:!1,emulateTouch:!0,renderArrowPrev:function(e,t,a){return t&&Object(g.jsx)("div",{style:{cursor:"pointer",position:"absolute",zIndex:1,top:"50%",transform:"translateY(-50%)",left:"10px",opacity:"0.7",backgroundColor:"white",borderRadius:"30px",width:"30px",height:"30px",display:"flex",justifyContent:"center",alignItems:"center"},onClick:e,children:Object(g.jsx)("img",{alt:"previous",style:{height:"20px",width:"20px",paddingRight:"2px"},src:n(369).default})})},renderArrowNext:function(e,t,a){return t&&Object(g.jsx)("div",{style:{cursor:"pointer",position:"absolute",zIndex:1,top:"50%",transform:"translateY(-50%)",right:"10px",opacity:"0.7",backgroundColor:"white",borderRadius:"30px",width:"30px",height:"30px",display:"flex",justifyContent:"center",alignItems:"center"},onClick:e,children:Object(g.jsx)("img",{alt:"next",style:{height:"20px",width:"20px",paddingLeft:"2px"},src:n(370).default})})}},"children",$.map((function(e){return Object(g.jsx)("img",{style:{borderRadius:"7px"},src:"/rvl-lab-utoronto.github.io/"+e,alt:"slideshow"})}))))}}]),a}(a.Component),te=function(e){Object(b.a)(n,e);var t=Object(m.a)(n);function n(){return Object(p.a)(this,n),t.apply(this,arguments)}return Object(u.a)(n,[{key:"render",value:function(){return Object(g.jsxs)(g.Fragment,{children:[Object(g.jsx)("div",{className:"desktop-view",children:Object(g.jsx)("div",{className:"horizontal-padding max-width-home",children:Object(g.jsxs)("div",{style:{display:"flex",flexDirection:"row"},children:[Object(g.jsxs)("div",{className:"left-section",children:[Object(g.jsx)(ee,{}),Object(g.jsx)("div",{style:{height:"20px"}}),Object(g.jsx)(J,{}),Object(g.jsx)("div",{style:{height:"20px"}}),Object(g.jsx)(B,{})]}),Object(g.jsx)("div",{className:"news-section",children:Object(g.jsx)(q,{})})]})})}),Object(g.jsx)("div",{className:"mobile-view",children:Object(g.jsxs)("div",{className:"horizontal-padding",children:[Object(g.jsx)(ee,{}),Object(g.jsx)(J,{}),Object(g.jsx)(B,{}),Object(g.jsx)(q,{})]})})]})}}]),n}(a.Component),ne=n(12),ae=n(108),ie=n(114),oe=n(432),re=n(434),se=n(73),ce=n(40),le=n.n(ce),de=[{question:"How can I join your lab as a postdoc?",answer:'Email <a href="http://www.cs.toronto.edu/~florian/">me</a> with your CV, unofficial transcripts from your graduate and undergraduate studies, your research interests, and a link to your publications. Take a look at these <a href="https://www.sgs.utoronto.ca/awards-category/postdoctoral-awards/">postdoc funding opportunities</a>. If you are also interested in simultaneously collaborating with an industrial partner, then take a look at the <a href="https://www.mitacs.ca/en/programs/elevate#postdoc">Mitacs Elevate</a> and <a href="https://www.mitacs.ca/en/programs/accelerate">Mitacs Accelerate</a> program. There are also many <a href="https://brancoweissfellowship.org/">other</a> <a href="https://www.queensu.ca/postdoc/prospective-scholars/external-funding-sources-funding-and-fellowships"> external funding opportunities</a>. Whatever your funding situation may be, if I see that we are a strong research match, I will do my best to find funding for you.'},{question:"How can I join your lab as a PhD student?",answer:'Feel free to email <a href="http://www.cs.toronto.edu/~florian/">me</a> with your CV, unofficial transcripts from your studies, and your research interests. If I see that we are a good research match I will reply and encourage you to submit an application. Unfortunately, I can not reply to all such emails because there are simply way too many. You should follow the application instructions for the PhD program at the <a href="https://web.cs.toronto.edu/graduate/msc-phd">Department of Computer Science at the University of Toronto</a>. You will be asked to list up to five potential supervisors when you apply. You should list my name, as well as robotics as one of your three research areas to ensure that I will see your application. Typically, the application deadline is in December for students applying to start in September of the following year. I will follow up with one or two Zoom interviews around end of December or January. If you get admitted to UofT CS you will have guaranteed funding for 4 years if you already have a MSc degree, or 5 years if you do not. That funding comes partially from the CS department, partially from my grants, and partially from any scholarships that you might receive.'},{question:"What types of scholarship opportunities are there for PhD students?",answer:'Canadian PhD applicants should definitely consider applying for the NSERC CGS-D award during the month of September, two months before they submit their application. In addition to that, there are many scholarship opportunities and entrance awards that are available for Canadian and international students. Take a look at this <a href="https://web.cs.toronto.edu/graduate/entrance-awards">list</a>.'},{question:"How can I join your lab as a MSc student?",answer:"Follow the exact same process outlined above for PhD applicants. If you get admitted to UofT CS you will have guaranteed funding for 1.5 years. This is the case both for Canadian and for international students. That funding comes partially from the CS department, partially from my grants, and partially from any scholarships that you might receive."},{question:"What types of scholarship opportunities are there for MSc students?",answer:'Canadian MSc applicants should definitely consider applying for the NSERC CGS-M award during the month of December, roughly the same time they submit their application. In addition to that, there are many scholarship opportunities and entrance awards that are available for Canadian students. Take a look at this <a href="https://web.cs.toronto.edu/graduate/entrance-awards">list</a>.'},{question:"Does getting a scholarship increase my chances of admission?",answer:"The scholarships that I mentioned above are awarded in late spring, after admission offers have gone out. So, receiving a scholarship is a bonus for your funding package after you get admitted, but it does not affect the admission decision."},{question:"How can I maximize my chances of MSc/PhD admission?",answer:"The more of these criteria apply to you, the better: (1) you have done well in mathematically demanding courses, (2) you have done well in robotics, machine learning, or computer vision courses, (3) you have done research before, (4) you have strong software engineering skills, (5) you have read and understood relevant research papers, (6) you have industry experience, (7) you have excellent reference letters, (8) you are really persistent and eager to learn more about the field, (9) you can work well both independently and in a team setting."},{question:"Is it ok if I email you to express interest in a MSc/PhD position in your lab?",answer:"I welcome emails from prospective students, however, I might not be able to answer your email. I do reply to messages from prospective students who seem like a good fit for my lab and I encourage them to apply. If I haven't replied to your message, it's probably because I don't see a strong fit with my lab's research."},{question:"What background reading should I do to understand your lab's research?",answer:'Check out this <a href="https://github.com/rvl-lab-utoronto/lab_onboarding_recommended_reading">recommended reading list</a> for new students who join my lab. It covers the set of all themes that current students in my lab are interested in and discuss daily. Note that you do not need to be interested in or understand all of these areas, but at least one.'},{question:"I am an undergraduate student at UofT. Can I do my thesis/capstone project with you?",answer:'I enjoy supervising about 3 undergraduate thesis/capstone students every year. You are expected to have taken or audited one of the <a href="http://www.cs.toronto.edu/~florian/">courses that I teach</a>, or some of the AI-related courses in CS or Engineering.'},{question:"I am an undergraduate student at UofT. Can I do summer research in your lab?",answer:'Our lab offers multiple opportunities for undergraduate students to do paid summer research. You are expected to have taken or audited one of the <a href="http://www.cs.toronto.edu/~florian/">courses that I teach</a>, or some of the AI-related courses in CS or Engineering. If that is not the case, you are expected to have done well in math-heavy courses or have software engineering skills. If you are interested in applying for these positions, you should apply for NSERC USRA, UofT UTEA, Engineering ESROP awards, or Mitacs RTA awards. The deadlines for many of these awards are in mid-January, so please contact me in December. If you do not get any of these awards, I will try to find other ways to pay you, but I will have significantly fewer such positions every year.'},{question:"I am a student at another university. Can I visit your lab or do research remotely?",answer:'If you think we are a strong research match and if you are planning to dedicate at least 4 months to your visit then feel free to email me with your CV, unofficial transcript, and research interests. You should also check out funding opportunities for visiting students, such as <a href="https://www.mitacs.ca/en/programs/globalink/globalink-research-award">Mitacs Globalink</a> or <a href="https://www.sgs.utoronto.ca/admissions/international-students/international-visiting-graduate-students/"> opportunities for international visiting graduate students at UofT</a>.'}],he=["header"],pe=function(e){var t=e.header,n=Object(ae.a)(e,he);return Object(g.jsx)(oe.a,Object(ne.a)(Object(ne.a)({},n),{},{header:Object(g.jsxs)(g.Fragment,{children:[t,Object(g.jsx)("img",{className:le.a.chevron,src:se.default,alt:"Chevron Down"})]}),className:le.a.item,buttonProps:{className:function(e){var t=e.isEnter;return"".concat(le.a.itemBtn," ").concat(t&&le.a.itemBtnExpanded)}},contentProps:{className:le.a.itemContent},panelProps:{className:le.a.itemPanel}}))},ue=function(e){Object(b.a)(n,e);var t=Object(m.a)(n);function n(e){var a;return Object(p.a)(this,n),(a=t.call(this,e)).state={readme:""},a}return Object(u.a)(n,[{key:"render",value:function(){return Object(g.jsx)("div",{className:"center",children:Object(g.jsxs)("div",{className:"horizontal-padding max-width",children:[Object(g.jsx)("h2",{children:"Prospective students"}),Object(g.jsx)("p",{children:"I am always looking for excellent students and researchers at all levels: postdocs, PhD, MSc, and undergraduates with computer science or engineering backgrounds. Our lab is an intellectually vibrant and a socially inclusive environment, and I enjoy working closely with my students, both graduate and undergraduate. When you join RVL, you will also have the opportunity to closely interact with my colleagues at the UofT CS Robotics Group as well as their students. We have very active members and weekly reading groups, as well as state-of-the-art facilities for robotics research. You will be part of a large, vibrant and healthy community."}),Object(g.jsx)("h2",{children:"How to join the lab: FAQ"}),Object(g.jsx)("div",{className:le.a.accordion,children:Object(g.jsx)(re.a,{transition:!0,transitionTimeout:250,allowMultiple:!0,children:de.map((function(e){return Object(g.jsx)(pe,{header:e.question,children:Object(g.jsx)(w.a,{rehypePlugins:[ie.a],children:e.answer})})}))})})]})})}}]),n}(a.Component),be=[{title:"Autonomous robots for environmental monitoring",description:"",image:"assets/project-assets/images/placeholder1.jpg",asset:"assets/project-assets/pages/autonomous-robots.md",webLocation:"autonomous-robots"},{title:"Learning to plan, perceive, and control",description:"",image:"assets/project-assets/images/placeholder2.jpg",asset:"assets/project-assets/pages/machine-learning.md",webLocation:"machine-learning"},{title:"Safe robot learning",description:"",image:"assets/project-assets/images/placeholder3.jpg",asset:"assets/project-assets/pages/safe-robot.md",webLocation:"safe-robot"}],me=(n(386),function(e){Object(b.a)(n,e);var t=Object(m.a)(n);function n(){return Object(p.a)(this,n),t.apply(this,arguments)}return Object(u.a)(n,[{key:"render",value:function(){var e=Object(g.jsxs)("div",{className:"project-box",children:[Object(g.jsx)("img",{alt:this.props.project.name,className:"project-image",src:"/rvl-lab-utoronto.github.io/"+this.props.project.image}),Object(g.jsx)("div",{style:{display:"flex",flexDirection:"column",justifyContent:"center"},children:Object(g.jsxs)("div",{className:"project-box-content",children:[Object(g.jsx)("h3",{style:{margin:0},children:this.props.project.title}),void 0!==this.props.project.description&&""!==this.props.project.description?Object(g.jsx)("p",{dangerouslySetInnerHTML:{__html:this.props.project.description}}):Object(g.jsx)(g.Fragment,{})]})})]});return void 0!==this.props.project.link&&""!==this.props.project.link?Object(g.jsx)("a",{href:this.props.project.link,className:"no-decoration",children:e}):void 0!==this.props.project.asset&&""!==this.props.project.asset&&void 0!==this.props.project.webLocation&&""!==this.props.project.webLocation?Object(g.jsx)(c.b,{className:"no-decoration",to:"/research/"+this.props.project.webLocation,children:e}):Object(g.jsx)("div",{className:"no-hover",children:e})}}]),n}(a.Component)),ge=function(e){Object(b.a)(n,e);var t=Object(m.a)(n);function n(){return Object(p.a)(this,n),t.apply(this,arguments)}return Object(u.a)(n,[{key:"render",value:function(){return Object(g.jsx)(F,{src:this.props.src,removeExtraSpace:!0})}}]),n}(a.Component),je=n(63),fe={default:3,985:2,600:1},ve=function(e){Object(b.a)(n,e);var t=Object(m.a)(n);function n(){return Object(p.a)(this,n),t.apply(this,arguments)}return Object(u.a)(n,[{key:"render",value:function(){return Object(g.jsx)("div",{className:"center",children:Object(g.jsxs)("div",{className:"horizontal-padding max-width",children:[Object(g.jsx)("div",{style:{height:"10px"}}),Object(g.jsx)(je.a,{breakpointCols:fe,className:"masonry-grid",children:be.map((function(e){return Object(g.jsx)(me,{project:e})}))})]})})}}]),n}(a.Component),ye=(n(387),function(e){Object(b.a)(n,e);var t=Object(m.a)(n);function n(){return Object(p.a)(this,n),t.apply(this,arguments)}return Object(u.a)(n,[{key:"render",value:function(){var e=this;return Object(g.jsx)(g.Fragment,{children:Object(g.jsx)("input",{value:this.props.value,style:Object(ne.a)(Object(ne.a)({},this.props.style),{},{fontSize:"15px"}),type:"text",placeholder:this.props.placeholder,onChange:function(t){return e.props.onChange(t.target.value)}})})}}]),n}(a.Component)),xe=[{bibtex:"\n\n       @InProceedings{Wei_2023_CVPR,\n          author    = {Cong Wei and Brendan Duke and Ruowei Jiang and Parham Aarabi and Graham Taylor and Florian Shkurti},\n          title     = {Sparsifiner: Learning Sparse Instance-Dependent Attention for Efficient Vision Transformers},\n          booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n          month     = {June},\n          year      = {2023},\n          pages     = {22680-22689}\n       }         \n    ",html:"https://arxiv.org/abs/2303.13755",tags:["computer vision"],thumbnail:"/assets/publication-thumbnails/cvpr23-sparsifiner.png",description:""},{bibtex:"\n\n      @InProceedings{Gu_2023_CVPR,\n         author    = {Qiao Gu and Dongsub Shim and Florian Shkurti},\n         title     = {Preserving Linear Separability in Continual Learning by Backward Feature Projection},\n         booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n         month     = {June},\n         year      = {2023},\n         pages     = {24286-24295}\n      }\n        \n    ",html:"https://arxiv.org/abs/2303.14595",tags:["computer vision","robot vision","continual learning"],code:"https://github.com/rvl-lab-utoronto/BFP",thumbnail:"/assets/publication-thumbnails/cvpr23-bfp.png",description:""},{bibtex:"\n\n      @article{jatavallabhula2023conceptfusion,\n          title={ConceptFusion: Open-set Multimodal 3D Mapping}, \n          author={Krishna Murthy Jatavallabhula and Alihusein Kuwajerwala and Qiao Gu and Mohd Omama and Tao Chen and Shuang Li and Ganesh Iyer and Soroush Saryazdi and Nikhil Keetha and Ayush Tewari and Joshua B. Tenenbaum and Celso Miguel de Melo and Madhava Krishna and Liam Paull and Florian Shkurti and Antonio Torralba},\n          year={2023},\n      booktitle={Robotics: Science and Systems (RSS)},\n      }\n        \n    ",html:"https://arxiv.org/abs/2302.07241",tags:["computer vision","robot vision"],code:"https://concept-fusion.github.io/",video:"https://concept-fusion.github.io/",thumbnail:"/assets/publication-thumbnails/rss23-conceptfusion.gif",description:""},{bibtex:"\n        \n       @article{Khodeir2023PolicyGuidedLS,\n         title={Policy-Guided Lazy Search with Feedback for Task and Motion Planning},\n         author={Mohamed Khodeir and Atharv Sonwane and Ruthrash Hari and Florian Shkurti},\n         booktitle={IEEE International Conference on Robotics and Automation (ICRA)},\n         year={2023},\n       }\n    ",html:"https://arxiv.org/abs/2210.14055",tags:["task and motion planning","manipulation"],code:"https://github.com/rvl-lab-utoronto/policy-guided-lazy-tamp",data:"https://github.com/rvl-lab-utoronto/policy-guided-lazy-tamp",video:"https://www.youtube.com/watch?v=HjSZOVkXSLU&t=1s&ab_channel=MohamedKhodeir",thumbnail:"/assets/publication-thumbnails/icra23-policy-guided-tamp.png",description:""},{bibtex:"\n      @article{https://doi.org/10.48550/arxiv.2111.13144,\n       author = {Mohamed Khodeir and Ben Agro and Florian Shkurti},\n       title = {Learning to Search in Task and Motion Planning with Streams},\n       journal = {Robotics and Automation Letters (RA-L)},\n       year = {2023},\n       volume={abs/2111.13144}\n    }\n\n    ",html:"https://arxiv.org/abs/2111.13144",tags:["task and motion planning","manipulation"],video:"https://rvl.cs.toronto.edu/learning-based-tamp/",code:"https://rvl.cs.toronto.edu/learning-based-tamp/",data:"https://rvl.cs.toronto.edu/learning-based-tamp/",thumbnail:"/assets/publication-thumbnails/ral23-learning-to-search.png",description:""},{bibtex:"\n \n        @article{Huang2022StochasticPF,\n           title={Stochastic Planning for ASV Navigation Using Satellite Images},\n           author={Yizhou Huang and Hamza Dugmag and Tim D. Barfoot and Florian Shkurti},\n           booktitle={IEEE International Conference on Robotics and Automation (ICRA)},\n           year={2023},\n        }\n\n    ",html:"https://arxiv.org/abs/2209.11864",tags:["planning","field robotics","robot vision"],code:"https://pcctp.github.io/",data:"https://pcctp.github.io/",video:"https://pcctp.github.io/",thumbnail:"/assets/publication-thumbnails/icra23-asv-navigation.png",description:""},{bibtex:"\n\n      @article{wang2023mvtrans,\n          title={MVTrans: Multi-View Perception of Transparent Objects}, \n          author={Yi Ru Wang and Yuchi Zhao and Haoping Xu and Saggi Eppel and Alan Aspuru-Guzik and Florian Shkurti and Animesh Garg},\n          year={2023},\n          booktitle={IEEE International Conference on Robotics and Automation (ICRA)},\n      }        \n    ",html:"https://arxiv.org/abs/2302.11683",tags:["computer vision","robot vision"],code:"https://ac-rad.github.io/MVTrans/",data:"https://ac-rad.github.io/MVTrans/",video:"https://www.youtube.com/watch?v=8Qdc_xWVp-k&ab_channel=XuHaoping",thumbnail:"/assets/publication-thumbnails/icra23-mvtrans.png",description:""},{bibtex:"\n      @article{https://doi.org/10.48550/arxiv.2212.09672,\n           author = {Naruki Yoshikawa and Andrew Zou Li and Kourosh Darvish and Yuchi Zhao and Haoping Xu and Alan Aspuru-Guzik and Animesh Garg and Florian Shkurti},\n           title = {Chemistry Lab Automation via Constrained Task and Motion Planning},\n           journal = {ArXiv},\n           year = {2022},\n           volume={abs/2212.09672}\n      }\n    ",html:"https://arxiv.org/abs/2212.09672",tags:["task and motion planning","planning","manipulation"],video:"https://ac-rad.github.io/arc-icra2023/",code:"https://ac-rad.github.io/arc-icra2023/",data:"https://ac-rad.github.io/arc-icra2023/",thumbnail:"/assets/publication-thumbnails/icra23-chemistry-tamp.gif",description:""},{bibtex:"\n      @inproceedings{xu2021seeing,\n        title={Seeing Glass: Joint Point-Cloud and Depth Completion for Transparent Objects},\n        author={Haoping Xu and Yi Ru Wang and Sagi Eppel and Alan Aspuru-Guzik and Florian Shkurti and Animesh Garg},\n        booktitle={Conference on Robot Learning (CoRL)},\n        year={2021},\n        url={https://openreview.net/forum?id=tCfLLiP7vje}\n      }\n    ",html:"https://openreview.net/forum?id=tCfLLiP7vje",tags:["robot vision"],video:"https://www.youtube.com/watch?v=SuUMKy52b4E&ab_channel=ConferenceonRobotLearning",code:"https://www.pair.toronto.edu/TranspareNet/",data:"https://borealisdata.ca/dataset.xhtml?persistentId=doi:10.5683/SP3/ZJJAJ3",thumbnail:"/assets/publication-thumbnails/todd.png",description:""},{bibtex:"\n      @inproceedings{agia2021taskography,\n         title={Taskography: Evaluating robot task planning over large 3D scene graphs},\n         author={Christopher Agia and Krishna Murthy Jatavallabhula and Mohamed Khodeir and Ondrej Miksik and Vibhav Vineet and Mustafa Mukadam and Liam Paull and Florian Shkurti},\n         booktitle={Conference on Robot Learning (CoRL)},\n         year={2021},\n         url={https://openreview.net/forum?id=nWLt35BU1z_}\n     }\n    ",html:"https://taskography.github.io/",tags:["planning"],video:"https://www.youtube.com/watch?v=mM4v5hP4LdA&t=17s&ab_channel=KrishnaMurthy",code:"https://github.com/taskography",thumbnail:"/assets/publication-thumbnails/taskography.png",description:""},{bibtex:"\n      @InProceedings{Khorasgani_2022_CVPR,\n    author    = {Salar Hosseini Khorasgani and Yuxuan Chen and Florian Shkurti},\n    title     = {SLIC: Self-Supervised Learning With Iterative Clustering for Human Action Videos},\n    booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n    month     = {June},\n    year      = {2022},\n    pages     = {16091-16101}\n    }\n    ",html:"https://openaccess.thecvf.com/content/CVPR2022/html/Khorasgani_SLIC_Self-Supervised_Learning_With_Iterative_Clustering_for_Human_Action_Videos_CVPR_2022_paper.html",tags:["computer vision"],video:"https://youtu.be/iTt4rOLdjCo",code:"https://github.com/rvl-lab-utoronto/video_similarity_search",thumbnail:"/assets/publication-thumbnails/slic.png",description:""},{bibtex:"\n      @inproceedings{equivariant_imitation_learning,\n        title={Augmenting Imitation Experience via Equivariant Representations},\n        author={Dhruv Sharma and Alihusein Kuwajerwala and Florian Shkurti},\n        year={2022},\n        pages={9383-9389},   \n        booktitle = {International Conference on Robotics and Automation (ICRA)}, \n      }\n    ",html:"https://arxiv.org/abs/2110.07668",tags:["robot vision","imitation learning"],video:"https://youtu.be/l-sCSj7PfmY",thumbnail:"/assets/publication-thumbnails/equivariant_imitation_sharma.png",description:""},{bibtex:"\n      @inproceedings{csc_homanga,\n        title={Conservative Safety Critics for Exploration},\n        author={Homanga Bharadhwaj and Aviral Kumar and Nicholas Rhinehart and Sergey Levine and Florian Shkurti and Animesh Garg},\n        year={2021},\n        booktitle = {International Conference on Learning Representations (ICLR)}, \n      }\n    ",html:"https://arxiv.org/abs/2010.14497",tags:["reinforcement learning","safe learning"],thumbnail:"/assets/publication-thumbnails/Conservative Safety Critics for Exploration.png",description:""},{bibtex:"\n      @inproceedings{gradsim,\n        title={gradSim: Differentiable simulation for system identification and visuomotor control },\n        author={Krishna Jatavallabhula and Miles Macklin and Florian Golemo and Vikram Voleti and Linda Petrini and Martin Weiss and Breandan Considine and Jerome Parent-Levesque and Kevin Xie and Kenny Erleben and Liam Paull and Florian Shkurti and Sanja Fidler and Derek Nowrouzezahrai},\n        year={2021},\n        booktitle = {International Conference on Learning Representations (ICLR)}, \n      }\n    ",html:"https://openreview.net/forum?id=c_E8kFWfhp0",tags:["simulation","differentiable rendering","system identification"],project:"https://gradsim.github.io/",thumbnail:"/assets/publication-thumbnails/gradSim Differentiable simulation for system identification and visuomotor control.png",description:""},{bibtex:"\n      @inproceedings{transferable_skills_kevin,\n        title={Skill Transfer via Partially Amortized Hierarchical Planning},\n        author={Kevin Xie and Homanga Bharadhwaj and Danijar Hafner and Animesh Garg and Florian Shkurti},\n        year={2021},\n        booktitle = {International Conference on Learning Representations (ICLR)},\n      }\n    ",html:"https://openreview.net/forum?id=jXe91kq3jAq",tags:["reinforcement learning"],thumbnail:"/assets/publication-thumbnails/Skill Transfer via Partially Amortized Hierarchical Planning.png",description:""},{bibtex:"\n      @inproceedings{sinha2020dibs,\n        title={DIBS: Diversity-Inducing Information Bottleneck in Model Ensembles},\n        author={Samarth Sinha and Homanga Bharadhwaj and Anirudh Goyal and Hugo Larochelle and Animesh Garg and Florian Shkurti},\n        year={2021},\n        booktitle = {Association for the Advancement of Artificial Intelligence (AAAI)}, \n      }\n    ",html:"https://arxiv.org/abs/2003.04514",tags:["generative models"],thumbnail:"/assets/publication-thumbnails/DIBS Diversity-Inducing Information Bottleneck in Model Ensembles.png",description:""},{bibtex:"\n      @inproceedings{koreitem2020oneshot,\n        title={One-Shot Informed Robotic Visual Search in the Wild},\n        author={Karim Koreitem and Florian Shkurti and Travis Manderson and Wei-Di Chang and Juan Camilo Gamboa Higuera and Gregory Dudek},\n        year={2020},\n        eprint={2003.10010},\n        archivePrefix={arXiv},\n        primaryClass={cs.RO},\n        booktitle={IEEE International Conference on Intelligent Robots and Systems (IROS)},\n      }\n    ",html:"https://arxiv.org/abs/2003.10010",code:"https://github.com/rvl-lab-utoronto/visual_search_in_the_wild",tags:["field robotics","robot vision","human-robot interaction"],thumbnail:"/assets/publication-thumbnails/One-Shot Informed Robotic Visual Search in the Wild.png",description:""},{bibtex:"\n      @INPROCEEDINGS{Manderson-RSS-20, \n        AUTHOR    = {Travis Manderson AND Juan Camilo Gamboa Higuera AND Stefan Wapnick AND Jean-Fran\xe7ois Tremblay AND Florian Shkurti AND David Meger AND Gregory Dudek}, \n        TITLE     = {{Vision-Based Goal-Conditioned Policies for Underwater Navigation in the Presence of Obstacles}}, \n        BOOKTITLE = {Robotics: Science and Systems (RSS)}, \n        YEAR      = {2020}, \n        ADDRESS   = {Corvalis, Oregon, USA}, \n        MONTH     = {July}, \n        DOI       = {10.15607/RSS.2020.XVI.048},\n      } \n    ",html:"http://www.roboticsproceedings.org/rss16/p048.html",video:"https://www.youtube.com/watch?v=qpcmwb_7QA4",project:"http://www.cim.mcgill.ca/mrl/nav2goal/",tags:["field robotics","robot vision","imitation learning"],thumbnail:"/assets/publication-thumbnails/Vision-Based Goal-Conditioned Policies for Underwater Navigation in the Presence of Obstacles.png",description:""},{bibtex:"\n      @inproceedings{bharadhwaj2020leaf,\n        title={LEAF: Latent Exploration Along the Frontier},\n        author={Homanga Bharadhwaj and Animesh Garg and Florian Shkurti},\n        year={2021},\n        eprint={2005.10934},\n        archivePrefix={arXiv},\n        primaryClass={cs.RO},\n        booktitle={IEEE International Conference on Robotics and Automation (ICRA)},\n      }\n    ",project:"https://sites.google.com/view/leaf-exploration",html:"https://arxiv.org/abs/2005.10934",tags:["reinforcement learning"],thumbnail:"/assets/publication-thumbnails/LEAF Latent Exploration Along the Frontier.png",description:""},{bibtex:"\n      @inproceedings{yuchen_wu_il_rl,\n        title={Shaping Rewards for Reinforcement Learning with Imperfect Demonstrations using Generative Models},\n        author={Yuchen Wu and Melissa Mozifian and Florian Shkurti},\n        year={2021},\n        eprint={2011.01298},\n        archivePrefix={arXiv},\n        primaryClass={cs.RO},\n        booktitle={IEEE International Conference on Robotics and Automation (ICRA)},\n      }\n    ",html:"https://arxiv.org/abs/2011.01298",tags:["reinforcement learning","imitation learning"],thumbnail:"/assets/publication-thumbnails/Shaping Rewards for Reinforcement Learning with Imperfect Demonstrations using Generative Models.png",description:""},{bibtex:"\n      @inproceedings{hypercrl,\n        title={Continual Model-Based Reinforcement Learning with Hypernetworks},\n        author={Yizhou Huang and Kevin Xie and Homanga Bharadhwaj and Florian Shkurti},\n        year={2021},\n        eprint={2009.11997},\n        archivePrefix={arXiv},\n        primaryClass={cs.RO},\n        booktitle={IEEE International Conference on Robotics and Automation (ICRA)},\n      }\n    ",html:"https://arxiv.org/abs/2009.11997",tags:["reinforcement learning","continual learning"],thumbnail:"/assets/publication-thumbnails/Continual Model-Based Reinforcement Learning with Hypernetworks.png",description:""},{bibtex:"\n      @inproceedings{loho,\n        title={LOHO: Latent Optimization of Hairstyles via Orthogonalization},\n        author={Rohit Saha and Brendan Duke and Florian Shkurti and Graham Taylor and Parham Aarabi},\n        year={2021},\n        eprint={2103.03891},\n        archivePrefix={arXiv},\n        primaryClass={cs.CV},\n        booktitle={Computer Vision and Pattern Recognition (CVPR)},\n      }\n    ",html:"https://arxiv.org/abs/2103.03891",tags:["generative models"],thumbnail:"/assets/publication-thumbnails/LOHO Latent Optimization of Hairstyles via Orthogonalization.png",description:""},{bibtex:"\n      @inproceedings{dong2020catch,\n        title={Catch the Ball: Accurate High-Speed Motions for Mobile Manipulators via Inverse Dynamics Learning},\n        author={Ke Dong and Karime Pereida and Florian Shkurti and Angela P. Schoellig},\n        year={2020},\n        eprint={2003.07489},\n        archivePrefix={arXiv},\n        primaryClass={cs.RO},\n        booktitle={IEEE International Conference on Intelligent Robots and Systems (IROS)},\n      }\n    ",html:"https://arxiv.org/abs/2003.07489",video:"https://www.youtube.com/watch?v=4uCvzurthS4",tags:["manipulation","control"],thumbnail:"/assets/publication-thumbnails/Catch the Ball Accurate High-Speed Motions for Mobile Manipulators via Inverse Dynamics Learning.png",description:""},{bibtex:"\n      @inproceedings{bharadhwaj2020modelpredictive,\n        title={Model-Predictive Control via Cross-Entropy and Gradient-Based Optimization},\n        author={Homanga Bharadhwaj and Kevin Xie and Florian Shkurti},\n        year={2020},\n        eprint={2004.08763},\n        archivePrefix={arXiv},\n        primaryClass={cs.LG},\n        booktitle={Conference on Learning for Dynamics and Control (L4DC)},\n      }\n    ",html:"https://arxiv.org/abs/2004.08763",tags:["planning","reinforcement learning"],thumbnail:"/assets/publication-thumbnails/Model-Predictive Control via Cross-Entropy and Gradient-Based Optimization.png",description:""},{bibtex:"\n      @article{Abeysirigoonawardena2019GeneratingAD,\n        title={Generating Adversarial Driving Scenarios in High-Fidelity Simulators},\n        author={Yasasa Abeysirigoonawardena and Florian Shkurti and Gregory Dudek},\n        journal={International Conference on Robotics and Automation (ICRA)},\n        year={2019},\n        pages={8271-8277},\n      }\n    ",project:"http://cim.mcgill.ca/~mrl/adversarial_driving_scenarios/",tags:["simulation","adversarial scenarios"],thumbnail:"/assets/publication-thumbnails/Generating Adversarial Driving Scenarios in High-Fidelity Simulators.png",description:""},{bibtex:'\n      @InProceedings{ florianICRA2018,\n        author = {Shkurti, Florian and Kakodkar, Nikhil and Dudek, Gregory},\n        title = {{Model-Based Probabilistic Pursuit via Inverse Reinforcement Learning}},\n        booktitle = "IEEE International Conference on Robotics and Automation (ICRA)",\n        year = "2018",\n        pages = "7804-7811",\n        month = "May",\n        address = "Brisbane, Australia",\n      }\n    ',video:"http://www.cim.mcgill.ca/~florian/pursuit_via_irl.mp4",pdf:"assets/pdf/icra_2018_irl_pursuit.pdf",tags:["planning","imitation learning"],thumbnail:"/assets/publication-thumbnails/Model-Based Probabilistic Pursuit via Inverse Reinforcement Learning.png",description:""},{bibtex:"\n      @INPROCEEDINGS{koreitemoceans18, \n        author={Karim Koreitem and Jimmy Li and Ian Karp and Travis Manderson and Florian Shkurti and Gregory Dudek}, \n        booktitle={IEEE OCEANS}, \n        title={Synthetically Trained 3D Visual Tracker of Underwater Vehicles}, \n        year={2018}, \n        volume={}, \n        number={}, \n        pages={1-7}, \n        month={Oct},\n      }\n    ",pdf:"assets/pdf/oceans18_synthetic_tracking.pdf",tags:["robot vision","field robotics","simulation"],thumbnail:"/assets/publication-thumbnails/Synthetically Trained 3D Visual Tracker of Underwater Vehicles.png",description:""},{bibtex:'\n      @InProceedings{convoying_iros_2017,\n        author = {Shkurti, Florian and Chang, {Wei Di} and Henderson, Peter and Islam, {Md. Jahidul} and {Gamboa Higuera}, {Juan Camilo} and Li, Jimmy and Manderson, Travis and Xu, Anqi and Dudek, Gregory and Sattar, Junaed},\n        title = "Underwater Multi-Robot Convoying Using Visual Tracking by Detection",\n        booktitle = "IEEE International Conference on Intelligent Robots and Systems (IROS)",\n        pages = "4189--4196",\n        year = "2017",\n        month = "September",\n        address = "Vancouver, Canada",\n        video= "http://www.cim.mcgill.ca/~mrl/robot_tracking/",\n      }\n    ',pdf:"assets/pdf/iros17_visual_convoying.pdf",tags:["robot vision","field robotics"],thumbnail:"/assets/publication-thumbnails/Underwater Multi-Robot Convoying Using Visual Tracking by Detection.png",description:""},{bibtex:'\n      @InProceedings{topological_pursuit_iros_2017,\n        author = "Florian Shkurti and Gregory Dudek",\n        title = "Topologically distinct trajectory predictions for probabilistic pursuit",\n        booktitle = "IEEE International Conference on Intelligent Robots and Systems (IROS)",\n        pages = "5653--5660",\n        year = "2017",\n        month = "September",\n        address = "Vancouver, Canada",\n      }\n    ',pdf:"assets/pdf/iros17_topological_pursuit.pdf",tags:["planning"],thumbnail:"/assets/publication-thumbnails/Topologically distinct trajectory predictions for probabilistic pursuit.png",description:""},{bibtex:"\n      @article{benchmark_environments_multitask,\n        author    = {Peter Henderson and\n                    Wei{-}Di Chang and\n                    Florian Shkurti and\n                    Johanna Hansen and\n                    David Meger and\n                    Gregory Dudek},\n        title     = {Benchmark Environments for Multitask Learning in Continuous Domains},\n        journal   = {CoRR},\n        volume    = {abs/1708.04352},\n        year      = {2017},\n        archivePrefix = {arXiv},\n        eprint    = {1708.04352},\n      }\n    ",html:"https://arxiv.org/abs/1708.04352",tags:["reinforcement learning","simulation"],thumbnail:"/assets/publication-thumbnails/Benchmark Environments for Multitask Learning in Continuous Domains.png",description:""},{bibtex:"\n      @inproceedings{manderson_crv_2016,\n        Author = {Manderson, Travis and Shkurti, Florian and Dudek, Gregory},\n        Booktitle = {Conference on Computer and Robot Vision (CRV)},\n        Month = {May},\n        Publisher = {IEEE Computer Society},\n        Title = {Texture-Aware SLAM Using Stereo Imagery And Inertial Information},\n              Pages = {465--463},\n              Year = {2016},\n      }\n    ",pdf:"assets/pdf/crv_2016_texture_aware_slam.pdf",tags:["field robotics","estimation","robot vision"],thumbnail:"/assets/publication-thumbnails/Texture-Aware SLAM Using Stereo Imagery And Inertial Information.png",description:""},{bibtex:"\n      @inproceedings{iros2014_megerShkurtiCortesPozaGiguereDudek,\n        author = {David Meger and Florian Shkurti and David Cort'{e}s Poza and Philippe Gigu`{e}re and Gregory Dudek},\n        title = {3D Trajectory Synthesis and Control for a Legged Swimming Robot},\n        booktitle = {Proceedings of the IEEE International Conference on Robotics and Intelligent Systems (IROS)},\n        year = {2014},\n      }\n    ",pdf:"assets/pdf/iros2014_3d_autopilot.pdf",project:"http://www.cim.mcgill.ca/~dmeger/IROS2014_3DTrajectories/",tags:["field robotics","control"],thumbnail:"/assets/publication-thumbnails/3D Trajectory Synthesis and Control for a Legged Swimming Robot.png",description:""},{bibtex:'\n      @InProceedings{Qiwen14iros,\n        author = {Zhang, Qiwen and Whitney, David and Shkurti, Florian and Rekleitis, Ioannis},\n        title = {{Ear-based Exploration on Hybrid Metric/Topological Maps}},\n        booktitle = "IEEE International Conference on Intelligent Robots and Systems (IROS)",\n        pages = "3081--3088",\n        year = "2014",\n        month = "September",\n        address = "Chicago, USA",\n\n      }\n    ',pdf:"assets/pdf/iros2014_gvg.pdf",tags:["planning"],thumbnail:"/assets/publication-thumbnails/Ear-based Exploration on Hybrid MetricTopological Maps.png",description:""},{bibtex:"\n      @inproceedings{Meghjani:2014:ARS:2623380.2623579,\n        author = {Meghjani, Malika and Shkurti, Florian and Higuera, Juan Camilo Gamboa and Kalmbach, Arnold and Whitney, David and Dudek, Gregory},\n        title = {Asymmetric Rendezvous Search at Sea},\n        booktitle = {Conference on Computer and Robot Vision (CRV)},\n        year = {2014},\n        pages = {175--180},\n      }\n    ",pdf:"assets/pdf/crv2014_asymmetric_rendezvous.pdf",tags:["field robotics","planning","control"],thumbnail:"/assets/publication-thumbnails/Asymmetric Rendezvous Search at Sea.png",description:""},{bibtex:'\n      @InProceedings{ Florian2014MaxViz,\n        author = {Shkurti, Florian and Dudek, Gregory},\n        title = {{Maximizing Visibility in Collaborative Trajectory Planning}},\n        booktitle = "IEEE International Conference on Robotics and Automation (ICRA)",\n        pages = "3771-3776",\n        year = "2014",\n        month = "May",\n        address = "Hong Kong",\n      }\n    ',pdf:"assets/pdf/icra2014_max_viz_planning.pdf",tags:["planning"],thumbnail:"/assets/publication-thumbnails/Maximizing Visibility in Collaborative Trajectory Planning.png",description:""},{bibtex:'\n      @InProceedings{Shkurti13icra,\n        author = {Shkurti, Florian and Dudek, Gregory},\n        title = {{On the Complexity of Searching for an Evader with a Faster Pursuer}},\n        booktitle = "IEEE International Conference on Robotics and Automation (ICRA)",\n        pages = "4047--4052",\n        year = "2013",\n        month = "May",\n        address = "Karlsruhe, Germany",\n      }\n    ',pdf:"assets/pdf/icra2013_complexity_pursuit_evasion.pdf",tags:["planning"],thumbnail:"/assets/publication-thumbnails/On the Complexity of Searching for an Evader with a Faster Pursuer.png",description:""},{bibtex:'\n      @InProceedings{Shkurti12iros,\n        author = {Shkurti, Florian and Xu, Anqi and Meghjani, Malika and {Gamboa Higuera}, {Juan Camilo} and  Girdhar, Yogesh and Giguere, Philippe and Dey, {Bir Bikram} and Li, Jimmy and Kalmbach, Arnold and Prahacs, Chris and Turgeon, Katrine and Rekleitis, Ioannis and Dudek, Gregory},\n        title = {{Multi-Domain Monitoring of Marine Environments Using a Heterogeneous Robot Team}},\n        booktitle = "IEEE International Conference on Intelligent Robots and Systems (IROS)",\n        pages = "1747--1753",\n        year = "2012",\n        month = "October",\n        address = "Algarve, Portugal",              \n      }\n    ',pdf:"assets/pdf/iros2012_multirobot_env_monitoring.pdf",project:"http://www.cim.mcgill.ca/~mrl/multi_robot_env_monitoring/",video:"https://www.youtube.com/watch?time_continue=1&v=DvWVC5R0zqs",tags:["field robotics"],thumbnail:"/assets/publication-thumbnails/Multi-Domain Monitoring of Marine Environments Using a Heterogeneous Robot Team.png",description:""},{bibtex:"\n      @InProceedings{ Gamboa2012SocialPlan,\n        author = {Juan Camilo Gamboa Higuera and Anqi Xu and Florian Shkurti and Gregory Dudek},\n        title = {Socially-Driven Collective Path Planning for Robot Missions},\n        booktitle = {Conference on Computer and Robot Vision (CRV)},\n        year = {2012},\n        pages = {417--424},\n        address = {Toronto, Canada},\n        month = {May},\n      }\n    ",pdf:"assets/pdf/crv2012_social_plan.pdf",tags:["planning"],thumbnail:"/assets/publication-thumbnails/Socially-Driven Collective Path Planning for Robot Missions.png",description:""},{bibtex:'\n      @InProceedings{florian_state_est,\n        author = {Shkurti, Florian and Rekleitis, Ioannis and Scaccia, Milena and Dudek, Gregory},\n        title = "{State estimation of an underwater robot using visual and inertial information}",\n        booktitle = "IEEE International Conference on Intelligent Robots and Systems (IROS)",\n        pages = "5054--5060",\n        year = "2011",\n        month = "September",\n        address = "San Francisco, USA",\n      }\n    ',pdf:"assets/pdf/iros_2011_state_est.pdf",tags:["estimation","field robotics"],thumbnail:"/assets/publication-thumbnails/State estimation of an underwater robot using visual and inertial information.png",description:""},{bibtex:'\n      @InProceedings{ Girdhar2011MARE,\n        author = "Yogesh Girdhar and Anqi Xu and Bir Bikram Dey and Malika Meghjani and Florian Shkurti and Ioannis Rekleitis and Gregory Dudek",\n        title = "{MARE: Marine Autonomous Robotic Explorer}",\n        booktitle = "IEEE International Conference on Intelligent Robots and Systems (IROS)",\n        pages = "5048--5053",\n        year = "2011",\n        month = "September",\n        address = "San Francisco, USA",\n      }\n    ',pdf:"assets/pdf/iros2011_boat.pdf",tags:["field robotics","robot vision"],thumbnail:"/assets/publication-thumbnails/MARE Marine Autonomous Robotic Explorer.png",description:""},{bibtex:"\n      @inproceedings{florian_crv_2011,\n        Author = {Shkurti, Florian and Rekleitis, Ioannis and Dudek, Gregory},\n        Booktitle = {Conference on Computer and Robot Vision},\n        Month = {May},\n        Publisher = {IEEE Computer Society},\n        Title = {Feature Tracking Evaluation for Pose Estimation in Underwater Environments},\n              Pages = {160--167},\n              Year = {2011},\n      }\n    ",pdf:"assets/pdf/crv2011_underwater_ft.pdf",tags:["field robotics","robot vision"],thumbnail:"/assets/publication-thumbnails/Feature Tracking Evaluation for Pose Estimation in Underwater Environments.png",description:""}],we=(n(388),n(110));function Oe(e){var t={pdf:"pdf",html:"pdf",bibtex:"bibtex",video:"video",project:"project",code:"code"},n=Object(we.a)(),a=n.getCollapseProps,i=n.getToggleProps,o=Object(g.jsx)(g.Fragment,{});return void 0!==e.publication.pdf&&void 0!==e.publication.thumbnail?o=Object(g.jsx)("a",{href:"/rvl-lab-utoronto.github.io/"+e.publication.pdf,children:Object(g.jsx)("img",{className:"publication-thumbnail",alt:e.publication.title,src:"/rvl-lab-utoronto.github.io/"+e.publication.thumbnail})}):void 0!==e.publication.html&&void 0!==e.publication.thumbnail?o=Object(g.jsx)("a",{href:e.publication.html,children:Object(g.jsx)("img",{className:"publication-thumbnail",alt:e.publication.title,src:"/rvl-lab-utoronto.github.io/"+e.publication.thumbnail})}):void 0!==e.publication.thumbnail&&(o=Object(g.jsx)("img",{className:"publication-thumbnail",alt:e.publication.title,src:"/rvl-lab-utoronto.github.io/"+e.publication.thumbnail})),Object(g.jsxs)("div",{className:"publication-entry",children:[e.showYear?Object(g.jsxs)(g.Fragment,{children:[Object(g.jsx)("hr",{}),Object(g.jsx)("h2",{children:e.publication.year})]}):Object(g.jsx)("div",{}),Object(g.jsxs)("div",{style:{display:"flex",flexDirection:"row",alignItems:"center"},children:[Object(g.jsx)("div",{className:"desktop-view",children:Object(g.jsx)("div",{style:{paddingRight:"25px"},children:o})}),Object(g.jsxs)("div",{children:[Object(g.jsx)("h3",{children:e.publication.title}),Object(g.jsx)("p",{children:e.publication.author}),""!==e.publication.booktitle?Object(g.jsx)("p",{children:Object(g.jsx)("span",{children:e.publication.booktitle})}):Object(g.jsx)("div",{}),""!==e.publication.journal?Object(g.jsx)("p",{children:Object(g.jsx)("span",{children:e.publication.journal})}):Object(g.jsx)("div",{}),void 0!==e.publication.tags?Object(g.jsx)("div",{children:e.publication.tags.map((function(t){return Object(g.jsx)(ke,{tag:t,selected:e.selectedTags.includes(t),addSelectedTag:e.addSelectedTag,removeSelectedTag:e.removeSelectedTag})}))}):Object(g.jsx)("div",{}),Object(g.jsx)("div",{style:{marginLeft:"-3px",marginTop:"3px"},children:Object.keys(t).map((function(n){return void 0!==e.publication[n]&&""!==e.publication[n]?"bibtex"===n?Object(g.jsxs)("div",Object(ne.a)(Object(ne.a)({style:{display:"inline",marginLeft:"3px"}},i()),{},{children:["[",Object(g.jsx)("div",{className:"a",style:{display:"inline"},children:"bibtex"}),"]"]}),n):"pdf"===n?Object(g.jsxs)("div",{style:{display:"inline",marginLeft:"3px"},children:["[",Object(g.jsx)("a",{href:"/rvl-lab-utoronto.github.io/"+e.publication[n],children:t[n]}),"]"]},n):Object(g.jsxs)("div",{style:{display:"inline",marginLeft:"3px"},children:["[",Object(g.jsx)("a",{href:e.publication[n],children:t[n]}),"]"]},n):Object(g.jsx)(g.Fragment,{})}))}),Object(g.jsx)("div",Object(ne.a)(Object(ne.a)({},a()),{},{children:Object(g.jsx)("div",{className:"bibtex-expand",children:e.publication.bibtex})}))]})]})]})}var ke=function(e){Object(b.a)(n,e);var t=Object(m.a)(n);function n(e){var a,i;return Object(p.a)(this,n),(i=t.call(this,e)).onClick=function(){i.state.selected?i.props.removeSelectedTag(i.props.tag):i.props.addSelectedTag(i.props.tag),i.setState({selected:!i.state.selected})},i.state={selected:null!==(a=i.props.selected)&&void 0!==a&&a},i}return Object(u.a)(n,[{key:"componentDidUpdate",value:function(e){this.props.selected!==e.selected&&this.setState({selected:this.props.selected})}},{key:"render",value:function(){return Object(g.jsx)("div",{onClick:this.onClick,className:"publication-tag "+(this.state.selected?"publication-tag-selected":""),children:this.props.tag})}}]),n}(a.Component),Se=(n(391),function(e){Object(b.a)(n,e);var t=Object(m.a)(n);function n(){return Object(p.a)(this,n),t.apply(this,arguments)}return Object(u.a)(n,[{key:"render",value:function(){return Object(g.jsxs)(g.Fragment,{children:[Object(g.jsx)("h1",{className:"page-header",children:this.props.title}),this.props.children,!1===this.props.showBreak?Object(g.jsx)("div",{}):Object(g.jsx)("hr",{})]})}}]),n}(a.Component)),Ce=n(111),Ie=n.n(Ce);function _e(e){if(!e.toLowerCase().includes(" and "))return e;for(var t=e.split(/ and /gi),n="",a=0;a<t.length;a++)n=a===t.length-1?n+" and "+t[a]:n+t[a]+", ";return n}function Ae(e){for(var t=e,n=(t=(t=(t=(t=(t=(t=(t=(t=(t=t.replaceAll("{{","{")).replaceAll("}}","}")).replace(/\s+/g," ")).replaceAll(" = ","=")).replaceAll(" =","=")).replaceAll("= ","=")).replaceAll('"{',"{")).replaceAll('}"',"}")).replaceAll("\n","")).toLowerCase(),a=["title","author","year","booktitle","journal"],i=0;i<a.length;i++){var o=", "+a[i]+"={",r=n.indexOf(o),s=n.indexOf(o)+o.length;-1!==r&&(t=t.replace(t.substring(r,s),t.substring(r,s).toLowerCase()))}return t}var Te=n(396),Re=function(e){Object(b.a)(n,e);var t=Object(m.a)(n);function n(e){var a;return Object(p.a)(this,n),(a=t.call(this,e)).filterPublications=function(){if(""===a.searchTerm&&0===a.selectedTags.length)return a.setState({shownPublications:a.state.publications}),void a.props.history.push({search:"?"+new URLSearchParams({}).toString()});0!==a.selectedTags.length&&""===a.searchTerm?a.props.history.push({search:"?"+new URLSearchParams({tags:JSON.stringify(a.selectedTags)}).toString()}):0===a.selectedTags.length&&""!==a.searchTerm?a.props.history.push({search:"?"+new URLSearchParams({search:JSON.stringify(a.searchTerm)}).toString()}):a.props.history.push({search:"?"+new URLSearchParams({tags:JSON.stringify(a.selectedTags),search:JSON.stringify(a.searchTerm)}).toString()});for(var e=a.state.publications,t=[],n=!1,i=0;i<e.length;i++){n=!1;for(var o=0;o<e[i].tags.length&&((a.selectedTags.includes(e[i].tags[o])||0===a.selectedTags.length)&&(""===a.searchTerm||e[i].title.toLowerCase().includes(a.searchTerm.toLowerCase())||e[i].author.toLowerCase().includes(a.searchTerm.toLowerCase())||e[i].booktitle.toLowerCase().includes(a.searchTerm.toLowerCase())||e[i].journal.toLowerCase().includes(a.searchTerm.toLowerCase()))&&(t.push(e[i]),n=!0),!n);o++);}a.setState({shownPublications:t})},a.getAllTags=function(e){for(var t=[],n=0;n<e.length;n++)if(void 0!==e[n].tags)for(var a=0;a<e[n].tags.length;a++)t.includes(e[n].tags[a])||t.push(e[n].tags[a]);return t.sort((function(e,t){var n=e.toUpperCase(),a=t.toUpperCase();return n<a?-1:n>a?1:0})),t},a.addSelectedTag=function(e){a.selectedTags.push(e),a.filterPublications()},a.removeSelectedTag=function(e){var t=Object(s.a)(a.selectedTags),n=t.indexOf(e);-1!==n&&(t.splice(n,1),a.selectedTags=t,a.filterPublications())},a.searchTerm="",a.selectedTags=[],a.state={publications:[],shownPublications:[],allTags:[]},a}return Object(u.a)(n,[{key:"componentDidMount",value:function(){var e=Object(x.a)(Object(v.a)().mark((function e(){var t,n,a,i,o,r,s,c,l,d,h,p,u,b,m,g,j=this;return Object(v.a)().wrap((function(e){for(;;)switch(e.prev=e.next){case 0:for(t=[],n=0;n<xe.length;n++)u=xe[n],b=Te.toJSON(Ae(xe[n].bibtex))[0],u.title=null!==(a=null===(i=b.entryTags)||void 0===i?void 0:i.title)&&void 0!==a?a:"",u.author=_e(null!==(o=null===(r=b.entryTags)||void 0===r?void 0:r.author)&&void 0!==o?o:""),u.year=null!==(s=null===(c=b.entryTags)||void 0===c?void 0:c.year)&&void 0!==s?s:"",u.booktitle=null!==(l=null===(d=b.entryTags)||void 0===d?void 0:d.booktitle)&&void 0!==l?l:"",u.journal=null!==(h=null===(p=b.entryTags)||void 0===p?void 0:p.journal)&&void 0!==h?h:"",t.push(u);t.sort((function(e,t){var n=e.booktitle.toUpperCase(),a=t.booktitle.toUpperCase();return n>a?-1:n<a?1:0})),t.sort((function(e,t){var n=e.year.toUpperCase(),a=t.year.toUpperCase();return n>a?-1:n<a?1:0})),m=this.getAllTags(t),this.setState({publications:t,shownPublications:t,allTags:m}),void 0===(g=Ie.a.parse(this.props.location.search)).tags&&void 0===g.search||(console.log(JSON.parse(g.tags)),void 0!==g.tags?this.selectedTags=JSON.parse(g.tags):this.selectedTags=[],void 0!==g.search?this.searchTerm=JSON.parse(g.search):this.searchTerm="",setTimeout((function(){j.filterPublications()}),1));case 8:case"end":return e.stop()}}),e,this)})));return function(){return e.apply(this,arguments)}}()},{key:"render",value:function(){var e=this,t="";return Object(g.jsx)(g.Fragment,{children:Object(g.jsx)("div",{className:"center",children:Object(g.jsxs)("div",{className:"horizontal-padding max-width",children:[Object(g.jsxs)(Se,{showBreak:!1,title:"Publications",children:[Object(g.jsx)("div",{style:{marginRight:"30px"},children:Object(g.jsx)(ye,{value:this.searchTerm,onChange:function(t){e.searchTerm=t,e.filterPublications()},placeholder:"Search title, author name, or publication venue"})}),void 0!==this.state.allTags?Object(g.jsx)("div",{children:this.state.allTags.map((function(t){return Object(g.jsx)(ke,{tag:t,addSelectedTag:e.addSelectedTag,removeSelectedTag:e.removeSelectedTag,selected:e.selectedTags.includes(t)})}))}):Object(g.jsx)("div",{})]}),this.state.shownPublications.map((function(n,a){return n.year!==t?(t=n.year,Object(g.jsx)(g.Fragment,{children:Object(g.jsx)(Oe,{showYear:!0,publication:n,selectedTags:e.selectedTags,addSelectedTag:e.addSelectedTag,removeSelectedTag:e.removeSelectedTag},a)})):Object(g.jsx)(g.Fragment,{children:Object(g.jsx)(Oe,{publication:n,selectedTags:e.selectedTags,addSelectedTag:e.addSelectedTag,removeSelectedTag:e.removeSelectedTag},a)})}))]})})})}}]),n}(a.Component),Me=Object(l.g)(Re),Pe={faculty:[{name:"Florian Shkurti",image:"assets/team/florian.jpg",description:"robot vision, machine learning, planning and control",website:"http://www.cs.toronto.edu/~florian/",twitter:"https://twitter.com/florian_shkurti",googleScholar:"https://scholar.google.com/citations?user=BDmtLHsAAAAJ&hl=en"}],Postdocs:[{name:"Kourosh Darvish",image:"assets/team/kourosh_darvish_2.jpg",description:"robotics, control, task and motion planning, machine learning, chemistry lab automation. co-supervised by <a href='https://animesh.garg.tech/'>Animesh Garg</a>",website:"https://kouroshd.github.io/",email:"kdarvish@cs.toronto.edu",linkedIn:"https://it.linkedin.com/in/kouroshdarvish",googleScholar:"https://scholar.google.com/citations?user=FwFFVdIAAAAJ"},{name:"Miroslav Bogdanovic",image:"assets/team/profile.png",description:"robotics, control, imitation and reinforcement learning. co-supervised by <a href='https://animesh.garg.tech/'>Animesh Garg</a>",website:"",email:"",linkedIn:"",googleScholar:""}],"PhD students":[{name:"Kevin Xie",image:"assets/team/kevin.jpg",description:"reinforcement learning, control, 3D vision. co-supervised by <a href='https://www.cs.utoronto.ca/~fidler/'>Sanja Fidler</a>",website:"https://kevincxie.github.io/",email:"kevincxie@cs.toronto.edu"},{name:"Qiao Gu",image:"assets/team/qiaogu.png",website:"https://georgegu1997.github.io/",email:"qiaog@andrew.cmu.edu",googleScholar:"https://scholar.google.com/citations?user=MF7ISVAAAAAJ&hl=en",description:"continual learning, computer vision"},{name:"Skylar Hao",image:"assets/team/skylar.png",description:"sim-to-real transfer, safe learning, statistics, machine learning"},{name:"Wei-Cheng Tseng",image:"assets/team/weicheng_tseng_1.jpg",description:"multi-agent reinforcement learning, computer vision",website:"https://weichengtseng.github.io/"},{name:"Sepehr Samavi",image:"assets/team/sepehr_samavi_1.jpg",description:"safe interactive navigation. co-supervised by <a href='https://www.dynsyslab.org/prof-angela-schoellig/'>Angela Schoellig</a>",website:"http://dsl.utias.utoronto.ca/~sep/",email:"sepehr@robotics.utias.utoronto.ca",googleScholar:"https://scholar.google.ca/citations?user=_j5XWygAAAAJ&hl=en"}],"MSc students":[{name:"Anthony Lem",image:"assets/team/anthony_lem_1.jpg",description:"human pose detection and prediction",email:"anthony.lem@mail.utoronto.ca"},{name:"Mohamed Khodeir",image:"assets/team/mk.png",linkedIn:"https://ca.linkedin.com/in/khodeir",description:"learning to plan, task and motion planning"},{name:"Andrei Ivanovic",image:"assets/team/andrei_ivanovic_2.jpeg",description:"trajectory prediction and planning. co-supervised by <a href='https://www.gilitschenski.org/igor/'>Igor Gilitschenski</a>",linkedIn:"https://ca.linkedin.com/in/andrei-ivanovic-438313178"},{name:"Yewon Lee",image:"assets/team/yewon_lee_1.jpeg",description:"task and motion planning",website:"https://yewon-lee.github.io/",email:"yewonlee@cs.toronto.edu"},{name:"Yasasa Abeysirigoonawardena",image:"assets/team/profile.png",description:"adversarial scenario generation, neural rendering",linkedIn:"https://ca.linkedin.com/in/yasasa-abeysirigoonawardena-819229198"},{name:"Jinbang Huang",image:"assets/team/profile.png",description:"task and motion planning, control theory, optimization. co-supervised by <a href='http://stars.utias.utoronto.ca/~jkelly/'>Jonathan Kelly</a>",linkedIn:"https://ca.linkedin.com/in/jinbang-huang-989526170"}],"undergraduate students":[{name:"Hamza Dugmag",image:"assets/team/hamza_dugmag_1.jpg",description:"field robotics, autonomous boat",website:"https://hamzadugmag.com/"},{name:"Jisu Qian",image:"assets/team/jisu_qian_1.jpeg",description:"system identification",linkedIn:"https://ca.linkedin.com/in/jisu-qian-85b18921b"},{name:"Andrew Zou Li",image:"assets/team/andrew_z_li.jpg",description:"task and motion planning, chemistry lab automation",website:"https://andrewzl.github.io/",linkedIn:"https://www.linkedin.com/in/andrewzouli/"},{name:"Yuchi(Allan) Zhao",image:"assets/team/yuchi_allan_zhao_1.jpg",description:"robot manipulation, transparent object pose estimation, task and motion planning, chemistry lab automation",website:"https://y556zhao.github.io/",linkedIn:"https://www.linkedin.com/in/yuchi-allan-zhao/"}],alumni:[{name:"Haozhe Sheng",image:"assets/team/profile.png",description:"<b>next: Google</b>"},{name:"Julia Chae",image:"assets/team/julia_chae_1.jpg",description:"LiDAR and RGB representation learning <b>next: MIT EECS PhD</b>",linkedIn:"https://ca.linkedin.com/in/julia-chae",website:"https://juliachae.github.io/"},{name:"Philip Huang",image:"assets/team/philip.jpg",description:"continual learning, field robotics, task and motion planning. co-supervised by <a href='http://asrl.utias.utoronto.ca/~tdb/'>Tim Barfoot</a>. <b>next: CMU CS PhD</b>",linkedIn:"https://ca.linkedin.com/in/philip-yizhou-huang",website:"https://philip-huang.github.io/",googleScholar:"https://scholar.google.com/citations?hl=en&user=YDCsS5EAAAAJ"},{name:"Ben Agro",image:"assets/team/ben_agro.jpg",description:"learning to plan, task and motion planning, manipulation. <b>next: UofT CS PhD / Waabi</b>",website:"https://benagro314.github.io/",twitter:"https://twitter.com/BenAgro4",email:"ben.agro@mail.utoronto.ca"},{name:"Aditya Saigal",image:"assets/team/profile.png",linkedIn:"https://ca.linkedin.com/in/aditya-saigal-221207143?trk=pub-pbmap",description:"continual reinforcement learning"},{name:"Salar Hosseini",image:"assets/team/salar.jpg",description:"computer vision, adversarial scenario generation. <b>next: Samsung AI</b>",linkedIn:"https://ca.linkedin.com/in/salar-hosseini",website:"https://salarios77.github.io/",googleScholar:"https://scholar.google.ca/citations?user=8OT5mY0AAAAJ&hl=en"},{name:"Cathlyn Chen",image:"assets/team/cathlyn_chen_1.jpg",description:"backwards reachability for nonlinear systems",link:""},{name:"Alex Alexiev",image:"assets/team/alex_alexiev_1.jpeg",description:"task and motion planning",linkedIn:"https://alex-alexiev.github.io/"},{name:"Kathy Zhuang",image:"assets/team/kathy_zhuang_1.jpeg",description:"computer vision for transparent objects. <b>next: Berkeley CS MSc</b>",linkedIn:"https://ca.linkedin.com/in/yue-kathy-zhuang",website:"https://kathyzhuang.github.io/"},{name:"Artur Kuramshin",image:"assets/team/artur_kuramshin_1.jpeg",description:"field robotics, autonomous boat",website:"http://akuramshin.ca",email:"artur.kuramshin@mail.utoronto.ca",linkedIn:"https://www.linkedin.com/in/artur-kuramshin-4b926616a/"},{name:"Ruiqi Wang",image:"assets/team/ruiqi_wang_1.jpeg",description:"adversarial scenario generation. <b>next: Stanford CS MSc</b>",linkedIn:"https://www.linkedin.com/in/ruiqi-wang-3b970b150/"},{name:"Charlotte Zhang",image:"assets/team/profile.png",description:"field robotics, autonomous boat"},{name:"Jason Tang",image:"assets/team/profile.png",description:"continual image classification. <b>next: UofT CS MSc</b>"},{name:"Pranit Chawla",image:"assets/team/pranit_chawla_1.jpg",description:"LiDAR and RGB representation for imitation learning. <b>next: CMU CS MSc</b>",website:"https://www.pranitchawla.com/",linkedIn:"https://www.linkedin.com/in/pranit-chawla-503237145/"},{name:"Cong Wei",image:"assets/team/cong_wei_1.jpeg",linkedIn:"https://ca.linkedin.com/in/cong-wei-30",description:"video summarization, generative models <b>next: Waterloo CS PhD</b>"},{name:"Kamran Ramji",image:"assets/team/kamran_ramji_1.jpg",description:"combining imitation and reinforcement learning. <b>next: Apple</b>",linkedIn:"https://ca.linkedin.com/in/kamran-ramji"},{name:"Stephen Zhao",image:"assets/team/stephen_zhao_1.jpeg",description:"multi-agent reinforcement learning. co-supervised by <a href='http://www.cs.toronto.edu/~yangxu/'>Yang Xu</a>. <b>next: UofT CS MSc</b>",linkedIn:"https://ca.linkedin.com/in/zhaostephen"},{name:"Ke Dong",image:"assets/team/ke_dong_1.jpg",description:"control theory, learning for control, robotics, sim-to-real transfer. co-supervised by <a href='https://www.dynsyslab.org/prof-angela-schoellig/'>Angela Schoellig</a>. <b>next: Tencent AI</b>",linkedIn:"https://ca.linkedin.com/in/ke-dong-7a33a9171"},{name:"Dhruv Sharma",image:"assets/team/dhruv_sharma_1.png",description:"autonomous driving, robotics, computer vision. <b>next: Huawei</b>",linkedIn:"https://ca.linkedin.com/in/dhruvsharmauw",website:"https://sharmadhruv.weebly.com/",email:"dhruv.sharma@uwaterloo.ca"},{name:"Sherry Chen",image:"assets/team/sherry_chen_1.jpg",description:"video representation learning. <b>next: UTIAS MSc</b>",linkedIn:"https://ca.linkedin.com/in/sherry-chen-engsci127"},{name:"Sally Chen",image:"assets/team/sally_chen_1.jpg",description:"differentiable rendering for self-driving simulators. <b>next: CMU CS MSc</b>",linkedIn:"https://ca.linkedin.com/in/chuhan-chen"},{name:"Homanga Bharadhwaj",image:"assets/team/homanga_bharadhwaj_1.png",description:"reinforcement learning, safe exploration, robotics, recommender systems. co-supervised by <a href='https://animesh.garg.tech/'>Animesh Garg</a>. <b>next: CMU CS PhD</b>",website:"https://homangab.github.io/",email:"homangablackhole36@gmail.com",googleScholar:"https://scholar.google.ca/citations?user=wwW4HRQAAAAJ&hl=en",twitter:"https://twitter.com/mangahomanga"},{name:"Chris Agia",image:"assets/team/chris_agia_1.jpg",description:"3d vision for autonomous driving. <b>next: Stanford CS PhD</b>",website:"https://agiachris.github.io/",email:"cagia@stanford.edu",linkedIn:"https://www.linkedin.com/in/agiachris/"},{name:"Ali Kuwajerwala",image:"assets/team/ali_kuwajerwala_1.jpg",description:"backwards reachability for nonlinear systems. <b>next: MILA MSc</b>",linkedIn:"https://ca.linkedin.com/in/alikuwajerwala"},{name:"Melissa Mozifian",image:"assets/team/melissa_mozifian_1.jpg",description:"visiting phd student, McGill/MILA. combining imitation and reinforcement learning. <b>visitor: MILA, McGill</b>",website:"https://melfm.github.io/about.html",googleScholar:"https://scholar.google.com/citations?user=sygJEU0AAAAJ&hl=en"},{name:"Shichen Lu",image:"assets/team/shichen_lu_1.jpeg",description:"control and reinforcement learning for mobile manipulation. <b>next: UTIAS MSc</b",linkedIn:"https://ca.linkedin.com/in/shichen-lu"},{name:"Siyun Li",image:"assets/team/profile.png",description:"adversarial examples for self-driving simulators. <b>next: Stanford CS MSc</b>",linkedIn:"https://ca.linkedin.com/in/siyun-li"},{name:"Yuchen Wu",image:"assets/team/yuchen_wu_1.jpg",description:"combining imitation and reinforcement learning. <b>next: UTIAS MSc</b>",linkedIn:"https://ca.linkedin.com/in/yuchen-wu-a9838a14a"},{name:"Zidong Weng",image:"assets/team/profile.png",description:"out-of-distribution detection for image and lidar data. <b>next: Intel</b>",linkedIn:"https://ca.linkedin.com/in/zidong-weng-232035134"},{name:"Zihan Wang",image:"assets/team/zihan_wang_1.jpg",description:"imitation learning for robotics. <b>next: Stanford CS MSc</b>",website:"https://avinwangzh.github.io/Personal-Website/",email:"avin.wangzihan@gmail.com",linkedIn:"https://ca.linkedin.com/in/zihan-wang-70aa47ab"}]},Le=(n(397),function(e){Object(b.a)(n,e);var t=Object(m.a)(n);function n(){return Object(p.a)(this,n),t.apply(this,arguments)}return Object(u.a)(n,[{key:"render",value:function(){var e=this,t=Object(g.jsx)("div",{className:"team-member-box",children:Object(g.jsxs)("div",{style:{display:"flex",flexDirection:"row"},children:[Object(g.jsx)("div",{children:Object(g.jsx)("img",{alt:this.props.teamMember.name,className:"team-member-image",src:"/rvl-lab-utoronto.github.io/"+this.props.teamMember.image})}),Object(g.jsxs)("div",{style:{display:"flex",flexDirection:"column",justifyContent:"center"},children:[Object(g.jsx)("h3",{className:"team-member-title",children:this.props.teamMember.name}),this.props.teamMember.description?Object(g.jsx)("p",{dangerouslySetInnerHTML:{__html:this.props.teamMember.description}}):Object(g.jsx)(g.Fragment,{}),Object(g.jsx)("div",{className:"team-member-socials",children:["website","email","twitter","linkedIn","googleScholar"].map((function(t){return void 0!==e.props.teamMember[t]?Object(g.jsx)(De,{social:t,teamMember:e.props.teamMember}):Object(g.jsx)(g.Fragment,{})}))})]})]})});return void 0!==this.props.teamMember.link&&""!==this.props.teamMember.link?Object(g.jsx)("a",{href:this.props.teamMember.link,className:"no-decoration",children:t}):Object(g.jsx)("div",{className:"no-hover",children:t})}}]),n}(a.Component)),De=function(e){Object(b.a)(n,e);var t=Object(m.a)(n);function n(){return Object(p.a)(this,n),t.apply(this,arguments)}return Object(u.a)(n,[{key:"render",value:function(){return Object(g.jsx)("a",{href:("email"===this.props.social?"mailto:":"")+this.props.teamMember[this.props.social],children:Object(g.jsx)("img",{className:"team-member-social-icon",alt:this.props.social,src:Ee(this.props.social)})})}}]),n}(a.Component);function Ee(e){switch(e){case"website":default:return n(96).default;case"email":return n(398).default;case"twitter":return n(399).default;case"linkedIn":return n(400).default;case"googleScholar":return n(401).default}}var Ne={default:3,1250:2,950:1},Fe=function(e){Object(b.a)(n,e);var t=Object(m.a)(n);function n(){return Object(p.a)(this,n),t.apply(this,arguments)}return Object(u.a)(n,[{key:"render",value:function(){for(var e=Object.keys(Pe),t=[],n=0;n<e.length;n++)t.push("title"),t.push(e[n]);return Object(g.jsx)("div",{className:"center",children:Object(g.jsxs)("div",{className:"horizontal-padding max-width",children:[Object(g.jsx)("div",{style:{height:"10px"}}),t.map((function(e,n){return"title"===e?Object(g.jsx)("h2",{style:{marginBottom:"5px",marginTop:"10px",textTransform:"capitalize"},children:t[n+1]},e):Object(g.jsx)(g.Fragment,{children:Object(g.jsx)(je.a,{breakpointCols:Ne,className:"masonry-grid",children:Pe[e].map((function(e){return Object(g.jsx)(Le,{teamMember:e},e.name)}))},e)})}))]})})}}]),n}(a.Component),Ge={main:[{title:"Home",link:"/",component:Object(g.jsx)(te,{})},{title:"Publications",link:"/publications",component:Object(g.jsx)(Me,{})},{title:"Team",link:"/team",component:Object(g.jsx)(Fe,{})},{title:"Research",link:"/research",component:Object(g.jsx)(ve,{})},{title:"Joining",link:"/joining",component:Object(g.jsx)(ue,{})},{title:"Blog",link:"/blog",component:Object(g.jsx)(z,{})}],hidden:[]},He=function(e){Object(b.a)(n,e);var t=Object(m.a)(n);function n(){return Object(p.a)(this,n),t.apply(this,arguments)}return Object(u.a)(n,[{key:"render",value:function(){return Object(g.jsx)("div",{style:{display:"flex",justifyContent:"center",alignItems:"center",height:"80vh",padding:"50px"},children:Object(g.jsx)("h1",{style:{fontSize:"50px",textAlign:"center"},children:"This page does not exist."})})}}]),n}(a.Component);function ze(){var e=i.a.useRef();function t(t){var n;null===(n=e.current)||void 0===n||n.handlePageChange(t)}return i.a.useEffect((function(){t(window.location.pathname)}),[]),Object(g.jsx)(c.a,{children:Object(g.jsx)(l.b,{render:function(n){var a=n.location;return Object(g.jsxs)("div",{style:{position:"absolute",right:0,left:0,bottom:0,top:0},children:[Object(g.jsx)(f,{}),Object(g.jsx)(A,{ref:e}),Object(g.jsx)(d.a,{component:"div",className:"App",children:Object(g.jsx)(h.a,{timeout:300,classNames:"page",children:Object(g.jsxs)(l.d,{location:a,children:[[].concat(Object(s.a)(Ge.main),Object(s.a)(Ge.hidden)).map((function(e,n){return Object(g.jsx)(l.b,{path:e.link,exact:!0,render:function(){return t(e.link),Object(g.jsxs)("div",{style:{position:"absolute",right:0,left:0,bottom:0,top:0},children:[Object(g.jsxs)("div",{style:{minHeight:"calc(100vh - 162px)"},children:["/blog"!==e.link?Object(g.jsx)(P,{}):Object(g.jsx)("div",{}),e.component]}),"/blog"!==e.link?Object(g.jsx)(G,{}):Object(g.jsx)("div",{})]})}},e.link)})),H.map((function(e){return void 0!==e.asset&&""!==e.asset&&void 0!==e.webLocation&&""!==e.webLocation?Object(g.jsx)(l.b,{path:"/blog/"+e.webLocation,exact:!0,render:function(){var n;t("/blog");var a=null===(n=e.asset)||void 0===n?void 0:n.includes(".html");return Object(g.jsxs)("div",{style:{position:"absolute",right:0,left:0,bottom:0,top:0},children:[Object(g.jsx)("div",{style:{minHeight:"100vh"},children:Object(g.jsx)(F,{articleData:e.articleData,distill:a,src:"/rvl-lab-utoronto.github.io/"+e.asset})}),a?Object(g.jsx)(g.Fragment,{}):Object(g.jsx)(G,{})]})}},e.title):Object(g.jsx)(l.b,{path:"/404",component:He},"404")})),be.map((function(e){return void 0!==e.asset&&""!==e.asset&&void 0!==e.webLocation&&""!==e.webLocation?Object(g.jsx)(l.b,{path:"/research/"+e.webLocation,exact:!0,render:function(){return t("/research"),Object(g.jsxs)("div",{style:{position:"absolute",right:0,left:0,bottom:0,top:0},children:[Object(g.jsx)("div",{style:{minHeight:"100vh"},children:Object(g.jsx)(ge,{src:"/rvl-lab-utoronto.github.io/"+e.asset})}),Object(g.jsx)(G,{})]})}},e.title):Object(g.jsx)(l.b,{path:"/404",component:He},"404")})),Object(g.jsx)(l.b,{path:"/404",component:He},"404"),Object(g.jsx)(l.a,{from:"*",to:"/"})]})},a.pathname)})]})}})})}var Ve=function(e){e&&e instanceof Function&&n.e(3).then(n.bind(null,441)).then((function(t){var n=t.getCLS,a=t.getFID,i=t.getFCP,o=t.getLCP,r=t.getTTFB;n(e),a(e),i(e),o(e),r(e)}))};r.a.render(Object(g.jsx)(i.a.StrictMode,{children:Object(g.jsx)(ze,{})}),document.getElementById("root")),Ve()},73:function(e,t,n){"use strict";n.r(t),t.default=n.p+"static/media/expand_more.8322b254.svg"},81:function(e,t,n){"use strict";n.r(t),t.default=n.p+"static/media/RVL-icon.0d3d35e6.png"},96:function(e,t,n){"use strict";n.r(t),t.default=n.p+"static/media/globe-solid.cecdc87b.svg"}},[[402,1,2]]]);
//# sourceMappingURL=main.b06f65b8.chunk.js.map